<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tutorial Interattivo: Transformer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }

        h1 {
            text-align: center;
            color: #667eea;
            margin-bottom: 10px;
            font-size: 2.5em;
        }

        .subtitle {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 1.1em;
        }

        .controls {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }

        .btn {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            transition: all 0.3s;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .btn-primary {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0,0,0,0.2);
        }

        .btn-secondary {
            background: #f0f0f0;
            color: #333;
        }

        .btn-secondary:hover {
            background: #e0e0e0;
        }

        .architecture {
            display: flex;
            justify-content: space-around;
            gap: 40px;
            margin: 40px 0;
            position: relative;
        }

        .column {
            flex: 1;
            display: flex;
            flex-direction: column;
            gap: 15px;
            align-items: center;
        }

        .column-title {
            font-size: 1.5em;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 10px;
        }

        .block {
            width: 100%;
            max-width: 300px;
            padding: 20px;
            border-radius: 12px;
            cursor: pointer;
            transition: all 0.3s;
            position: relative;
            border: 3px solid transparent;
            text-align: center;
        }

        .block:hover {
            transform: scale(1.05);
            box-shadow: 0 8px 16px rgba(0,0,0,0.2);
        }

        .block.active {
            border-color: #667eea;
            box-shadow: 0 0 20px rgba(102, 126, 234, 0.5);
        }

        .block.animating {
            animation: pulse 1s ease-in-out;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.08); }
        }

        .input-block { background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%); }
        .embedding-block { background: linear-gradient(135deg, #a1c4fd 0%, #c2e9fb 100%); }
        .positional-block { background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%); }
        .attention-block { background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%); }
        .feedforward-block { background: linear-gradient(135deg, #fbc2eb 0%, #a6c1ee 100%); }
        .norm-block { background: linear-gradient(135deg, #fdcbf1 0%, #e6dee9 100%); }
        .output-block { background: linear-gradient(135deg, #ffd89b 0%, #19547b 100%); color: white; }

        .arrow {
            width: 3px;
            height: 20px;
            background: #667eea;
            margin: 0 auto;
            position: relative;
        }

        .arrow::after {
            content: '';
            position: absolute;
            bottom: -8px;
            left: 50%;
            transform: translateX(-50%);
            width: 0;
            height: 0;
            border-left: 8px solid transparent;
            border-right: 8px solid transparent;
            border-top: 8px solid #667eea;
        }

        .info-panel {
            background: #f8f9fa;
            border-radius: 12px;
            padding: 25px;
            margin-top: 30px;
            min-height: 200px;
            border-left: 5px solid #667eea;
        }

        .info-panel h2 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1.8em;
        }

        .info-panel p {
            color: #555;
            line-height: 1.8;
            margin-bottom: 12px;
            font-size: 1.05em;
        }

        .info-panel ul {
            margin-left: 25px;
            color: #555;
            line-height: 1.8;
        }

        .info-panel li {
            margin-bottom: 8px;
        }

        .visualization {
            background: white;
            border-radius: 12px;
            padding: 25px;
            margin-top: 20px;
            border: 2px solid #e0e0e0;
        }

        .matrix {
            display: inline-grid;
            gap: 4px;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 8px;
            margin: 10px;
        }

        .matrix-cell {
            width: 40px;
            height: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            background: white;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-size: 12px;
            font-weight: bold;
            transition: all 0.3s;
        }

        .matrix-cell:hover {
            background: #667eea;
            color: white;
            transform: scale(1.1);
        }

        .token-flow {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin: 20px 0;
        }

        .token {
            padding: 10px 20px;
            background: #667eea;
            color: white;
            border-radius: 8px;
            font-weight: bold;
            animation: tokenFloat 2s ease-in-out infinite;
        }

        @keyframes tokenFloat {
            0%, 100% { transform: translateY(0px); }
            50% { transform: translateY(-10px); }
        }

        .attention-head {
            display: inline-block;
            width: 60px;
            height: 60px;
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            border-radius: 50%;
            margin: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            color: white;
            cursor: pointer;
            transition: all 0.3s;
        }

        .attention-head:hover {
            transform: scale(1.2) rotate(10deg);
        }

        .qkv-container {
            display: flex;
            justify-content: space-around;
            gap: 20px;
            margin: 20px 0;
            flex-wrap: wrap;
        }

        .qkv-matrix {
            text-align: center;
        }

        .qkv-label {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
            color: #667eea;
        }

        .matrix-3d {
            perspective: 1000px;
            display: inline-block;
        }

        .matrix-3d:hover .matrix {
            transform: rotateY(15deg) rotateX(10deg);
        }

        .attention-score {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            font-weight: bold;
            text-align: center;
        }

        .translation-demo {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            padding: 30px;
            border-radius: 15px;
            margin: 20px 0;
            color: white;
        }

        .translation-step {
            background: rgba(255,255,255,0.2);
            padding: 15px;
            border-radius: 10px;
            margin: 15px 0;
            backdrop-filter: blur(10px);
        }

        .sentence-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 20px;
            margin: 20px 0;
            flex-wrap: wrap;
        }

        .sentence-box {
            flex: 1;
            min-width: 250px;
            background: rgba(255,255,255,0.3);
            padding: 20px;
            border-radius: 12px;
            backdrop-filter: blur(10px);
        }

        .sentence-title {
            font-size: 1.1em;
            font-weight: bold;
            margin-bottom: 10px;
            text-transform: uppercase;
        }

        .word-token {
            display: inline-block;
            padding: 8px 15px;
            background: white;
            color: #667eea;
            border-radius: 6px;
            margin: 5px;
            font-weight: bold;
            transition: all 0.3s;
            cursor: pointer;
        }

        .word-token:hover {
            transform: scale(1.1) translateY(-3px);
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
        }

        .arrow-translation {
            font-size: 3em;
            color: white;
            animation: arrowPulse 2s ease-in-out infinite;
        }

        @keyframes arrowPulse {
            0%, 100% { transform: translateX(0); opacity: 0.6; }
            50% { transform: translateX(10px); opacity: 1; }
        }

        .progress-bar {
            width: 100%;
            height: 30px;
            background: rgba(255,255,255,0.3);
            border-radius: 15px;
            overflow: hidden;
            margin: 15px 0;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #84fab0 0%, #8fd3f4 100%);
            width: 0%;
            transition: width 0.5s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }

        .attention-heatmap {
            display: inline-grid;
            gap: 2px;
            margin: 20px auto;
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
        }

        .heatmap-cell {
            width: 50px;
            height: 50px;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 6px;
            font-size: 11px;
            font-weight: bold;
            color: white;
            transition: all 0.3s;
            cursor: pointer;
        }

        .heatmap-cell:hover {
            transform: scale(1.2);
            z-index: 10;
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
        }

        .word-label {
            font-weight: bold;
            color: #667eea;
            margin: 5px;
        }

        .code-example {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
        }

        .highlight {
            color: #a6e22e;
        }

        .legend {
            display: flex;
            gap: 20px;
            justify-content: center;
            flex-wrap: wrap;
            margin: 30px 0;
        }

        .legend-item {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .legend-color {
            width: 30px;
            height: 30px;
            border-radius: 6px;
        }

        /* New Styles for Interactive Features */
        .comparison-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }

        .comparison-box {
            background: white;
            border-radius: 15px;
            padding: 25px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            border: 3px solid #e0e0e0;
        }

        .comparison-box.transformer {
            border-color: #667eea;
        }

        .comparison-box.rnn {
            border-color: #f093fb;
        }

        .comparison-title {
            font-size: 1.5em;
            font-weight: bold;
            margin-bottom: 15px;
            text-align: center;
        }

        .vanishing-gradient-demo {
            display: flex;
            align-items: center;
            gap: 10px;
            margin: 20px 0;
        }

        .gradient-cell {
            width: 50px;
            height: 50px;
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            color: white;
            transition: all 0.5s;
        }

        .playground-input {
            width: 100%;
            padding: 15px;
            font-size: 1.1em;
            border: 2px solid #667eea;
            border-radius: 8px;
            margin: 15px 0;
        }

        .playground-controls {
            display: flex;
            gap: 15px;
            margin: 20px 0;
            flex-wrap: wrap;
        }

        .weight-slider {
            width: 100%;
            margin: 15px 0;
        }

        .slider-label {
            display: flex;
            justify-content: space-between;
            margin-bottom: 5px;
            color: #667eea;
            font-weight: bold;
        }

        .multi-head-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 20px;
            margin: 20px 0;
        }

        .head-card {
            background: white;
            border-radius: 12px;
            padding: 15px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            cursor: pointer;
            transition: all 0.3s;
            border: 2px solid transparent;
        }

        .head-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 16px rgba(0,0,0,0.2);
            border-color: #667eea;
        }

        .head-card.selected {
            border-color: #667eea;
            background: linear-gradient(135deg, #667eea15, #764ba215);
        }

        .head-title {
            font-weight: bold;
            color: #667eea;
            margin-bottom: 10px;
            text-align: center;
        }

        .head-description {
            font-size: 0.9em;
            color: #666;
            text-align: center;
            margin-bottom: 10px;
        }

        .canvas-3d {
            border: 2px solid #e0e0e0;
            border-radius: 10px;
            background: #f8f9fa;
            display: block;
            margin: 20px auto;
        }

        .training-chart {
            width: 100%;
            height: 300px;
            background: white;
            border-radius: 10px;
            border: 2px solid #e0e0e0;
            margin: 20px 0;
            position: relative;
        }

        .loss-curve {
            stroke: #667eea;
            stroke-width: 3;
            fill: none;
        }

        .epoch-marker {
            fill: #764ba2;
            cursor: pointer;
        }

        .quiz-container {
            background: white;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }

        .quiz-question {
            font-size: 1.2em;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 20px;
        }

        .quiz-options {
            display: flex;
            flex-direction: column;
            gap: 10px;
        }

        .quiz-option {
            padding: 15px;
            background: #f8f9fa;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s;
        }

        .quiz-option:hover {
            background: #667eea15;
            border-color: #667eea;
        }

        .quiz-option.correct {
            background: #84fab0;
            border-color: #84fab0;
            color: white;
        }

        .quiz-option.incorrect {
            background: #ff9a9e;
            border-color: #ff9a9e;
            color: white;
        }

        .draggable-item {
            padding: 15px;
            background: #667eea;
            color: white;
            border-radius: 8px;
            margin: 5px;
            cursor: move;
            user-select: none;
        }

        .drop-zone {
            min-height: 60px;
            background: #f8f9fa;
            border: 2px dashed #667eea;
            border-radius: 8px;
            padding: 10px;
            margin: 10px 0;
        }

        .drop-zone.drag-over {
            background: #667eea15;
        }

        .code-view-panel {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            max-height: 500px;
            overflow-y: auto;
        }

        .code-tabs {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
        }

        .code-tab {
            padding: 8px 16px;
            background: #444;
            border: none;
            border-radius: 5px;
            color: white;
            cursor: pointer;
            transition: all 0.3s;
        }

        .code-tab.active {
            background: #667eea;
        }

        .debug-panel {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
        }

        .tensor-view {
            background: #2d2d2d;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            border-left: 4px solid #667eea;
        }

        .tensor-label {
            color: #4ec9b0;
            font-weight: bold;
            margin-bottom: 10px;
        }

        .tensor-values {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(60px, 1fr));
            gap: 5px;
        }

        .tensor-value {
            background: #333;
            padding: 5px;
            border-radius: 4px;
            text-align: center;
            font-size: 0.85em;
        }

        .calculator-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .calculator-input-group {
            margin: 15px 0;
        }

        .calculator-label {
            font-weight: bold;
            color: #667eea;
            margin-bottom: 8px;
            display: block;
        }

        .calculator-input {
            width: 100%;
            padding: 10px;
            border: 2px solid #667eea;
            border-radius: 6px;
            font-size: 1em;
        }

        .calculator-result {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }

        .result-item {
            display: flex;
            justify-content: space-between;
            padding: 10px 0;
            border-bottom: 1px solid rgba(255,255,255,0.2);
        }

        .result-item:last-child {
            border-bottom: none;
        }

        .result-label {
            font-weight: bold;
        }

        .result-value {
            font-size: 1.1em;
        }

        .practical-examples {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .example-card {
            background: white;
            border-radius: 12px;
            padding: 20px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            cursor: pointer;
            transition: all 0.3s;
            border: 2px solid transparent;
        }

        .example-card:hover {
            transform: translateY(-5px);
            border-color: #667eea;
            box-shadow: 0 8px 16px rgba(0,0,0,0.2);
        }

        .example-icon {
            font-size: 3em;
            text-align: center;
            margin-bottom: 15px;
        }

        .example-title {
            font-size: 1.3em;
            font-weight: bold;
            color: #667eea;
            text-align: center;
            margin-bottom: 10px;
        }

        .example-description {
            color: #666;
            text-align: center;
            line-height: 1.6;
        }

        @media (max-width: 768px) {
            .comparison-container {
                grid-template-columns: 1fr;
            }
            
            .multi-head-grid {
                grid-template-columns: repeat(2, 1fr);
            }
            
            .calculator-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü§ñ Tutorial Interattivo: Architettura Transformer</h1>
        <p class="subtitle">Clicca su ogni componente per esplorare come funziona</p>

        <div class="controls">
            <button class="btn btn-primary" onclick="animateFlow()">‚ñ∂Ô∏è Anima Flusso Dati</button>
            <button class="btn btn-secondary" onclick="resetAnimation()">üîÑ Reset</button>
            <button class="btn btn-secondary" onclick="showOverview()">üìã Panoramica</button>
            <button class="btn btn-primary" onclick="showTranslationDemo()">üåç Demo Traduzione</button>
            <button class="btn btn-primary" onclick="showQKVVisualization()">üîç Visualizza Q, K, V</button>
            <button class="btn btn-primary" onclick="showRNNComparison()">üîÑ RNN vs Transformer</button>
            <button class="btn btn-primary" onclick="showAttentionPlayground()">üéÆ Playground Attention</button>
            <button class="btn btn-primary" onclick="showMultiHeadDetail()">üéØ Multi-Head Detail</button>
            <button class="btn btn-primary" onclick="showPositionalEncodingInteractive()">üìä Positional Encoding 3D</button>
            <button class="btn btn-primary" onclick="showTrainingVisualization()">üìà Training Visualization</button>
            <button class="btn btn-primary" onclick="showComplexityCalculator()">üßÆ Complexity Calculator</button>
            <button class="btn btn-primary" onclick="showQuizMode()">üéì Quiz</button>
            <button class="btn btn-secondary" onclick="toggleCodeView()">üíª Code View</button>
            <button class="btn btn-secondary" onclick="toggleDebugMode()">üêõ Debug Mode</button>
        </div>

        <div class="architecture">
            <div class="column">
                <div class="column-title">ENCODER</div>
                
                <div class="block input-block" onclick="showInfo('input')">
                    <strong>Input Tokens</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Sequenza di input</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block embedding-block" onclick="showInfo('embedding')">
                    <strong>Token Embedding</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Vettori d=512</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block positional-block" onclick="showInfo('positional')">
                    <strong>Positional Encoding</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Informazione posizionale</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block attention-block" onclick="showInfo('attention')">
                    <strong>Multi-Head Attention</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">8 teste parallele</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block norm-block" onclick="showInfo('norm')">
                    <strong>Add & Norm</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Residual + LayerNorm</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block feedforward-block" onclick="showInfo('feedforward')">
                    <strong>Feed Forward</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">FFN(x) = max(0, xW‚ÇÅ+b‚ÇÅ)W‚ÇÇ+b‚ÇÇ</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block norm-block" onclick="showInfo('norm2')">
                    <strong>Add & Norm</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Residual + LayerNorm</div>
                </div>
            </div>

            <div class="column">
                <div class="column-title">DECODER</div>
                
                <div class="block input-block" onclick="showInfo('output-input')">
                    <strong>Output Tokens</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Sequenza di output</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block embedding-block" onclick="showInfo('output-embedding')">
                    <strong>Token Embedding</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Vettori d=512</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block positional-block" onclick="showInfo('output-positional')">
                    <strong>Positional Encoding</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Informazione posizionale</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block attention-block" onclick="showInfo('masked-attention')">
                    <strong>Masked Multi-Head Attention</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Previene look-ahead</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block norm-block" onclick="showInfo('decoder-norm1')">
                    <strong>Add & Norm</strong>
                </div>
                <div class="arrow"></div>
                
                <div class="block attention-block" onclick="showInfo('cross-attention')">
                    <strong>Cross Attention</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Encoder-Decoder</div>
                </div>
                <div class="arrow"></div>
                
                <div class="block norm-block" onclick="showInfo('decoder-norm2')">
                    <strong>Add & Norm</strong>
                </div>
                <div class="arrow"></div>
                
                <div class="block feedforward-block" onclick="showInfo('decoder-ff')">
                    <strong>Feed Forward</strong>
                </div>
                <div class="arrow"></div>
                
                <div class="block norm-block" onclick="showInfo('decoder-norm3')">
                    <strong>Add & Norm</strong>
                </div>
                <div class="arrow"></div>
                
                <div class="block output-block" onclick="showInfo('final-output')">
                    <strong>Linear + Softmax</strong>
                    <div style="font-size: 0.9em; margin-top: 8px;">Predizione token</div>
                </div>
            </div>
        </div>

        <div class="legend">
            <div class="legend-item">
                <div class="legend-color input-block"></div>
                <span>Input/Output</span>
            </div>
            <div class="legend-item">
                <div class="legend-color embedding-block"></div>
                <span>Embedding</span>
            </div>
            <div class="legend-item">
                <div class="legend-color positional-block"></div>
                <span>Positional</span>
            </div>
            <div class="legend-item">
                <div class="legend-color attention-block"></div>
                <span>Attention</span>
            </div>
            <div class="legend-item">
                <div class="legend-color feedforward-block"></div>
                <span>Feed Forward</span>
            </div>
            <div class="legend-item">
                <div class="legend-color norm-block"></div>
                <span>Normalization</span>
            </div>
        </div>

        <div class="info-panel" id="infoPanel">
            <h2>Benvenuto! üëã</h2>
            <p>Questo √® un tutorial interattivo sull'architettura Transformer, introdotta nel paper "Attention is All You Need" (2017).</p>
            <p><strong>Come usare questo tutorial:</strong></p>
            <ul>
                <li>Clicca su qualsiasi blocco per vedere spiegazioni dettagliate</li>
                <li>Usa il pulsante "Anima Flusso Dati" per vedere come i dati si muovono attraverso la rete</li>
                <li>Esplora le visualizzazioni interattive per ogni componente</li>
            </ul>
            <p>I Transformer hanno rivoluzionato il NLP e sono alla base di modelli come GPT, BERT, e molti altri!</p>
        </div>

        <div class="visualization" id="visualization" style="display:none;"></div>
    </div>

    <script>
        const info = {
            'input': {
                title: 'Input Tokens',
                content: `
                    <p>L'input √® una sequenza di token (parole o subword) che rappresentano il testo da processare.</p>
                    <div class="token-flow">
                        <div class="token">Il</div>
                        <div class="token">gatto</div>
                        <div class="token">dorme</div>
                    </div>
                    <p><strong>Processo:</strong></p>
                    <ul>
                        <li>Il testo viene tokenizzato (diviso in unit√†)</li>
                        <li>Ogni token viene convertito in un ID numerico</li>
                        <li>Gli ID vengono passati al layer di embedding</li>
                    </ul>
                `
            },
            'embedding': {
                title: 'Token Embedding',
                content: `
                    <p>Ogni token viene mappato in uno spazio vettoriale di dimensione d=512 (nel Transformer originale).</p>
                    <p><strong>Esempio:</strong> "gatto" ‚Üí [0.2, -0.5, 0.8, ..., 0.1] (512 dimensioni)</p>
                    <div class="code-example">
<span class="highlight">embedding_matrix</span> = nn.Embedding(vocab_size, d_model)
output = <span class="highlight">embedding_matrix</span>(input_ids) * sqrt(d_model)
                    </div>
                    <p><strong>Perch√© moltiplicare per ‚àöd_model?</strong> Per stabilizzare la scala dei valori prima di aggiungere il positional encoding.</p>
                `
            },
            'positional': {
                title: 'Positional Encoding',
                content: `
                    <p>Siccome i Transformer non hanno ricorrenza, dobbiamo aggiungere informazioni sulla posizione dei token.</p>
                    <p><strong>Formula:</strong></p>
                    <div class="code-example">
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
                    </div>
                    <p>Dove:</p>
                    <ul>
                        <li><strong>pos</strong> = posizione del token nella sequenza</li>
                        <li><strong>i</strong> = dimensione nel vettore embedding</li>
                        <li><strong>d_model</strong> = dimensione del modello (512)</li>
                    </ul>
                    <p>Questo encoding permette al modello di apprendere facilmente le posizioni relative.</p>
                `
            },
            'attention': {
                title: 'Multi-Head Self-Attention',
                content: `
                    <p>Il cuore del Transformer! Permette ad ogni token di "guardare" tutti gli altri token e decidere quanto sono rilevanti.</p>
                    <div style="text-align: center; margin: 20px 0;">
                        <div class="attention-head">H1</div>
                        <div class="attention-head">H2</div>
                        <div class="attention-head">H3</div>
                        <div class="attention-head">H4</div>
                        <div class="attention-head">H5</div>
                        <div class="attention-head">H6</div>
                        <div class="attention-head">H7</div>
                        <div class="attention-head">H8</div>
                    </div>
                    <p><strong>Formula dell'Attention:</strong></p>
                    <div class="code-example">
Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V
                    </div>
                    <p><strong>Componenti:</strong></p>
                    <ul>
                        <li><strong>Q (Query):</strong> "Cosa sto cercando?"</li>
                        <li><strong>K (Key):</strong> "Cosa offro?"</li>
                        <li><strong>V (Value):</strong> "Qual √® il mio contenuto?"</li>
                    </ul>
                    <p>Le 8 teste lavorano in parallelo, ognuna imparando a catturare diversi tipi di relazioni (sintattiche, semantiche, ecc.).</p>
                    <button class="btn btn-primary" onclick="showQKVVisualization()" style="margin-top: 15px;">üîç Visualizza Q, K, V in dettaglio</button>
                `
            },
            'norm': {
                title: 'Add & Norm (Residual Connection + Layer Normalization)',
                content: `
                    <p>Combina due tecniche fondamentali per l'addestramento di reti profonde:</p>
                    <p><strong>1. Residual Connection (Skip Connection):</strong></p>
                    <div class="code-example">
output = LayerNorm(x + Sublayer(x))
                    </div>
                    <p>Permette al gradiente di fluire direttamente attraverso la rete, facilitando l'addestramento.</p>
                    <p><strong>2. Layer Normalization:</strong></p>
                    <ul>
                        <li>Normalizza le attivazioni lungo la dimensione delle feature</li>
                        <li>Stabilizza l'addestramento</li>
                        <li>Permette di usare learning rate pi√π alti</li>
                    </ul>
                `
            },
            'feedforward': {
                title: 'Position-wise Feed-Forward Network',
                content: `
                    <p>Una rete neurale fully-connected applicata indipendentemente a ogni posizione.</p>
                    <div class="code-example">
FFN(x) = max(0, xW‚ÇÅ + b‚ÇÅ)W‚ÇÇ + b‚ÇÇ
                    </div>
                    <p><strong>Architettura:</strong></p>
                    <ul>
                        <li>Input: 512 dimensioni</li>
                        <li>Hidden layer: 2048 dimensioni (espansione 4x)</li>
                        <li>Output: 512 dimensioni</li>
                        <li>Attivazione: ReLU</li>
                    </ul>
                    <p>Questo layer permette al modello di processare le informazioni raccolte dall'attention in modo non-lineare.</p>
                `
            },
            'masked-attention': {
                title: 'Masked Multi-Head Attention',
                content: `
                    <p>Simile alla self-attention, ma con una maschera che previene di guardare i token futuri.</p>
                    <p><strong>Perch√©?</strong> Durante il training, dobbiamo prevenire che il decoder "bari" guardando i token che deve predire!</p>
                    <div class="code-example">
# Maschera applicata prima del softmax
scores = QK^T / ‚àöd_k
scores = scores.masked_fill(mask == 0, -1e9)
attention = softmax(scores)
                    </div>
                    <p>La maschera √® una matrice triangolare inferiore che permette solo di guardare i token precedenti.</p>
                `
            },
            'cross-attention': {
                title: 'Cross Attention (Encoder-Decoder Attention)',
                content: `
                    <p>Questo √® dove l'encoder e il decoder si collegano!</p>
                    <p><strong>Differenza dalla Self-Attention:</strong></p>
                    <ul>
                        <li><strong>Q (Query):</strong> Viene dal decoder (output che stiamo generando)</li>
                        <li><strong>K, V (Key, Value):</strong> Vengono dall'encoder (input originale)</li>
                    </ul>
                    <div class="code-example">
# Il decoder "chiede" all'encoder
Q = decoder_output
K = encoder_output
V = encoder_output
attention = softmax(QK^T / ‚àöd_k) V
                    </div>
                    <p>Permette al decoder di "guardare" l'intera sequenza di input mentre genera l'output.</p>
                `
            },
            'final-output': {
                title: 'Linear + Softmax',
                content: `
                    <p>Gli ultimi layer convertono i vettori del decoder in probabilit√† sui token del vocabolario.</p>
                    <p><strong>Processo:</strong></p>
                    <div class="code-example">
# Linear projection
logits = output @ W_vocab  # [batch, seq_len, vocab_size]

# Softmax per ottenere probabilit√†
probabilities = softmax(logits)

# Scegli il token pi√π probabile
next_token = argmax(probabilities)
                    </div>
                    <p><strong>Durante l'inferenza:</strong> Il token predetto viene aggiunto alla sequenza e il processo continua autoregressivamente.</p>
                `
            },
            'norm2': {
                title: 'Add & Norm',
                content: `
                    <p>Seconda applicazione di residual connection e layer normalization dopo il feed-forward network.</p>
                    <p>Questo pattern (Attention ‚Üí Add&Norm ‚Üí FFN ‚Üí Add&Norm) viene ripetuto N volte (6 volte nel Transformer originale).</p>
                `
            },
            'output-input': {
                title: 'Output Tokens (Shifted Right)',
                content: `
                    <p>Durante il training, l'input del decoder √® la sequenza target shiftata di una posizione.</p>
                    <p><strong>Esempio:</strong></p>
                    <ul>
                        <li>Target: "Il gatto dorme"</li>
                        <li>Decoder input: "&lt;START&gt; Il gatto"</li>
                        <li>Decoder output: "Il gatto dorme"</li>
                    </ul>
                    <p>Questo permette al modello di imparare a predire il token successivo dato il contesto precedente.</p>
                `
            },
            'output-embedding': {
                title: 'Output Embedding',
                content: `
                    <p>Identico all'embedding dell'encoder. Spesso i pesi vengono condivisi tra encoder e decoder embedding.</p>
                    <p>Converte i token di output in vettori densi di 512 dimensioni.</p>
                `
            },
            'output-positional': {
                title: 'Output Positional Encoding',
                content: `
                    <p>Stesso positional encoding usato nell'encoder, applicato all'output embedding.</p>
                    <p>Cruciale per mantenere l'informazione sull'ordine dei token nella sequenza di output.</p>
                `
            },
            'decoder-norm1': {
                title: 'Add & Norm (dopo Masked Attention)',
                content: `<p>Residual connection e normalization dopo il layer di masked self-attention.</p>`
            },
            'decoder-norm2': {
                title: 'Add & Norm (dopo Cross Attention)',
                content: `<p>Residual connection e normalization dopo il layer di cross attention encoder-decoder.</p>`
            },
            'decoder-norm3': {
                title: 'Add & Norm (dopo FFN)',
                content: `<p>Ultima residual connection e normalization del decoder block.</p>`
            },
            'decoder-ff': {
                title: 'Feed Forward (Decoder)',
                content: `
                    <p>Identico al feed-forward network dell'encoder:</p>
                    <ul>
                        <li>512 ‚Üí 2048 ‚Üí 512 dimensioni</li>
                        <li>Attivazione ReLU</li>
                        <li>Applicato indipendentemente a ogni posizione</li>
                    </ul>
                `
            }
        };

        function showInfo(key) {
            const panel = document.getElementById('infoPanel');
            const vis = document.getElementById('visualization');
            
            // Remove active class from all blocks
            document.querySelectorAll('.block').forEach(b => b.classList.remove('active'));
            
            // Add active class to clicked block
            event.target.closest('.block').classList.add('active');
            
            if (info[key]) {
                panel.innerHTML = `<h2>${info[key].title}</h2>${info[key].content}`;
                vis.style.display = 'none';
            }
        }

        function showOverview() {
            const panel = document.getElementById('infoPanel');
            panel.innerHTML = `
                <h2>üìã Panoramica del Transformer</h2>
                <p><strong>Innovazioni chiave:</strong></p>
                <ul>
                    <li><strong>Self-Attention:</strong> Ogni token pu√≤ guardare tutti gli altri token, catturando dipendenze a lungo raggio</li>
                    <li><strong>Parallelizzazione:</strong> A differenza delle RNN, tutto pu√≤ essere processato in parallelo</li>
                    <li><strong>Multi-Head Attention:</strong> Cattura diversi tipi di relazioni simultaneamente</li>
                    <li><strong>Positional Encoding:</strong> Soluzione elegante per codificare la posizione senza ricorrenza</li>
                </ul>
                <p><strong>Applicazioni:</strong></p>
                <ul>
                    <li>Traduzione automatica (uso originale)</li>
                    <li>Generazione di testo (GPT)</li>
                    <li>Comprensione del linguaggio (BERT)</li>
                    <li>Computer Vision (Vision Transformer)</li>
                    <li>Speech recognition</li>
                    <li>Modelli multimodali</li>
                </ul>
                <p><strong>Complessit√†:</strong></p>
                <ul>
                    <li>Self-Attention: O(n¬≤¬∑d) dove n = lunghezza sequenza, d = dimensione modello</li>
                    <li>Feed-Forward: O(n¬∑d¬≤)</li>
                </ul>
                <p><strong>Parametri del Transformer originale:</strong> ~65M parametri per il modello base</p>
            `;
            document.querySelectorAll('.block').forEach(b => b.classList.remove('active'));
        }

        async function animateFlow() {
            const blocks = document.querySelectorAll('.block');
            const arrows = document.querySelectorAll('.arrow');
            
            // Reset
            blocks.forEach(b => b.classList.remove('animating'));
            
            // Encoder animation
            const encoderBlocks = Array.from(blocks).slice(0, 7);
            for (let i = 0; i < encoderBlocks.length; i++) {
                encoderBlocks[i].classList.add('animating');
                await sleep(500);
                encoderBlocks[i].classList.remove('animating');
            }
            
            // Decoder animation
            const decoderBlocks = Array.from(blocks).slice(7);
            for (let i = 0; i < decoderBlocks.length; i++) {
                decoderBlocks[i].classList.add('animating');
                await sleep(500);
                decoderBlocks[i].classList.remove('animating');
            }
            
            // Show completion message
            const panel = document.getElementById('infoPanel');
            panel.innerHTML = `
                <h2>‚úÖ Animazione Completata!</h2>
                <p>Hai appena visto il flusso di dati attraverso l'intera architettura Transformer:</p>
                <ul>
                    <li><strong>Encoder:</strong> Processa l'input e crea una rappresentazione contestuale</li>
                    <li><strong>Decoder:</strong> Usa la rappresentazione dell'encoder per generare l'output token per token</li>
                </ul>
                <p><strong>Nota:</strong> Nell'implementazione reale, l'encoder viene eseguito una volta, mentre il decoder viene eseguito autoregressivamente per ogni token di output.</p>
            `;
        }

        function resetAnimation() {
            document.querySelectorAll('.block').forEach(b => {
                b.classList.remove('animating', 'active');
            });
            showOverview();
        }

        function sleep(ms) {
            return new Promise(resolve => setTimeout(resolve, ms));
        }

        function showQKVVisualization() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üîç Visualizzazione Matrici Q, K, V</h2>
                <p>Ecco come funziona il meccanismo di attention in dettaglio con un esempio pratico!</p>
            `;
            
            vis.style.display = 'block';
            vis.innerHTML = `
                <h3 style="color: #667eea; text-align: center; margin-bottom: 20px;">Esempio: "Il gatto dorme"</h3>
                
                <div class="qkv-container">
                    <div class="qkv-matrix">
                        <div class="qkv-label">Query (Q)</div>
                        <div class="matrix-3d">
                            <div class="matrix" style="grid-template-columns: repeat(4, 1fr);">
                                <div class="matrix-cell" style="background: #667eea; color: white;">0.2</div>
                                <div class="matrix-cell" style="background: #667eea; color: white;">0.5</div>
                                <div class="matrix-cell" style="background: #667eea; color: white;">-0.3</div>
                                <div class="matrix-cell" style="background: #667eea; color: white;">0.8</div>
                                <div class="matrix-cell" style="background: #7c8ee8; color: white;">0.1</div>
                                <div class="matrix-cell" style="background: #7c8ee8; color: white;">-0.4</div>
                                <div class="matrix-cell" style="background: #7c8ee8; color: white;">0.6</div>
                                <div class="matrix-cell" style="background: #7c8ee8; color: white;">0.3</div>
                                <div class="matrix-cell" style="background: #929ee6; color: white;">-0.2</div>
                                <div class="matrix-cell" style="background: #929ee6; color: white;">0.7</div>
                                <div class="matrix-cell" style="background: #929ee6; color: white;">0.4</div>
                                <div class="matrix-cell" style="background: #929ee6; color: white;">-0.5</div>
                            </div>
                        </div>
                        <div style="margin-top: 10px; font-size: 0.9em; color: #666;">
                            "Cosa sto cercando?"<br>
                            [3 tokens √ó 4 dim]
                        </div>
                    </div>
                    
                    <div class="qkv-matrix">
                        <div class="qkv-label">Key (K)</div>
                        <div class="matrix-3d">
                            <div class="matrix" style="grid-template-columns: repeat(4, 1fr);">
                                <div class="matrix-cell" style="background: #f093fb; color: white;">0.3</div>
                                <div class="matrix-cell" style="background: #f093fb; color: white;">-0.2</div>
                                <div class="matrix-cell" style="background: #f093fb; color: white;">0.5</div>
                                <div class="matrix-cell" style="background: #f093fb; color: white;">0.1</div>
                                <div class="matrix-cell" style="background: #f3a0fb; color: white;">0.7</div>
                                <div class="matrix-cell" style="background: #f3a0fb; color: white;">0.4</div>
                                <div class="matrix-cell" style="background: #f3a0fb; color: white;">-0.3</div>
                                <div class="matrix-cell" style="background: #f3a0fb; color: white;">0.6</div>
                                <div class="matrix-cell" style="background: #f5adfb; color: white;">-0.1</div>
                                <div class="matrix-cell" style="background: #f5adfb; color: white;">0.8</div>
                                <div class="matrix-cell" style="background: #f5adfb; color: white;">0.2</div>
                                <div class="matrix-cell" style="background: #f5adfb; color: white;">-0.4</div>
                            </div>
                        </div>
                        <div style="margin-top: 10px; font-size: 0.9em; color: #666;">
                            "Cosa offro?"<br>
                            [3 tokens √ó 4 dim]
                        </div>
                    </div>
                    
                    <div class="qkv-matrix">
                        <div class="qkv-label">Value (V)</div>
                        <div class="matrix-3d">
                            <div class="matrix" style="grid-template-columns: repeat(4, 1fr);">
                                <div class="matrix-cell" style="background: #84fab0; color: white;">0.5</div>
                                <div class="matrix-cell" style="background: #84fab0; color: white;">0.2</div>
                                <div class="matrix-cell" style="background: #84fab0; color: white;">-0.6</div>
                                <div class="matrix-cell" style="background: #84fab0; color: white;">0.9</div>
                                <div class="matrix-cell" style="background: #8ffbb5; color: white;">0.3</div>
                                <div class="matrix-cell" style="background: #8ffbb5; color: white;">-0.5</div>
                                <div class="matrix-cell" style="background: #8ffbb5; color: white;">0.7</div>
                                <div class="matrix-cell" style="background: #8ffbb5; color: white;">0.1</div>
                                <div class="matrix-cell" style="background: #9afcba; color: white;">-0.3</div>
                                <div class="matrix-cell" style="background: #9afcba; color: white;">0.6</div>
                                <div class="matrix-cell" style="background: #9afcba; color: white;">0.4</div>
                                <div class="matrix-cell" style="background: #9afcba; color: white;">-0.8</div>
                            </div>
                        </div>
                        <div style="margin-top: 10px; font-size: 0.9em; color: #666;">
                            "Qual √® il mio contenuto?"<br>
                            [3 tokens √ó 4 dim]
                        </div>
                    </div>
                </div>
                
                <div style="text-align: center; margin: 30px 0;">
                    <div style="font-size: 2em; color: #667eea;">‚¨áÔ∏è</div>
                    <div class="attention-score">
                        Calcolo: QK^T / ‚àöd_k
                    </div>
                </div>
                
                <h3 style="color: #667eea; text-align: center; margin: 20px 0;">Attention Scores (dopo softmax)</h3>
                <div style="text-align: center;">
                    <div class="word-label">Query ‚Üí</div>
                    <div style="display: flex; justify-content: center; gap: 5px; margin: 10px 0;">
                        <div class="word-label"></div>
                        <div class="word-label">Il</div>
                        <div class="word-label">gatto</div>
                        <div class="word-label">dorme</div>
                    </div>
                    <div class="attention-heatmap" style="grid-template-columns: auto repeat(3, 50px);">
                        <div class="word-label" style="width: 60px;">Il</div>
                        <div class="heatmap-cell" style="background: #667eea;" title="Il ‚Üí Il: 0.45">0.45</div>
                        <div class="heatmap-cell" style="background: #8f9eeb;" title="Il ‚Üí gatto: 0.35">0.35</div>
                        <div class="heatmap-cell" style="background: #b8c4ee;" title="Il ‚Üí dorme: 0.20">0.20</div>
                        
                        <div class="word-label" style="width: 60px;">gatto</div>
                        <div class="heatmap-cell" style="background: #9aa9ec;" title="gatto ‚Üí Il: 0.30">0.30</div>
                        <div class="heatmap-cell" style="background: #5a6fe7;" title="gatto ‚Üí gatto: 0.55">0.55</div>
                        <div class="heatmap-cell" style="background: #c7d0f0;" title="gatto ‚Üí dorme: 0.15">0.15</div>
                        
                        <div class="word-label" style="width: 60px;">dorme</div>
                        <div class="heatmap-cell" style="background: #c7d0f0;" title="dorme ‚Üí Il: 0.15">0.15</div>
                        <div class="heatmap-cell" style="background: #7d8de9;" title="dorme ‚Üí gatto: 0.40">0.40</div>
                        <div class="heatmap-cell" style="background: #667eea;" title="dorme ‚Üí dorme: 0.45">0.45</div>
                    </div>
                </div>
                
                <div style="margin: 30px 0; padding: 20px; background: #f8f9fa; border-radius: 10px;">
                    <h4 style="color: #667eea; margin-bottom: 15px;">üí° Interpretazione:</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>Colori pi√π scuri = Attention pi√π forte:</strong> Il modello presta pi√π attenzione a quella relazione</li>
                        <li><strong>"gatto" ‚Üí "gatto" (0.55):</strong> Ogni parola presta molta attenzione a se stessa</li>
                        <li><strong>"gatto" ‚Üí "Il" (0.30):</strong> "gatto" guarda "Il" per capire che √® un sostantivo definito</li>
                        <li><strong>"dorme" ‚Üí "gatto" (0.40):</strong> "dorme" guarda "gatto" per capire chi sta dormendo</li>
                    </ul>
                </div>
                
                <div style="text-align: center; margin: 20px 0;">
                    <div style="font-size: 2em; color: #667eea;">‚¨áÔ∏è</div>
                    <div class="attention-score">
                        Output = Attention_Scores √ó V
                    </div>
                </div>
                
                <div style="background: linear-gradient(135deg, #84fab0, #8fd3f4); padding: 20px; border-radius: 10px; color: white; text-align: center;">
                    <h4 style="margin-bottom: 10px;">‚ú® Risultato Finale</h4>
                    <p>Ogni token ora ha una nuova rappresentazione che incorpora informazioni contestuali da tutti gli altri token, pesate in base alla loro rilevanza!</p>
                </div>
            `;
        }

        async function showTranslationDemo() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üåç Demo Traduzione: Inglese ‚Üí Italiano</h2>
                <p>Vediamo passo-passo come un Transformer traduce una frase!</p>
            `;
            
            vis.style.display = 'block';
            vis.innerHTML = `
                <div class="translation-demo">
                    <h3 style="text-align: center; margin-bottom: 20px;">Traduzione in tempo reale</h3>
                    
                    <div class="sentence-container">
                        <div class="sentence-box">
                            <div class="sentence-title">üá¨üáß Input (Inglese)</div>
                            <div id="sourceTokens">
                                <div class="word-token">The</div>
                                <div class="word-token">cat</div>
                                <div class="word-token">sleeps</div>
                            </div>
                        </div>
                        
                        <div class="arrow-translation">‚Üí</div>
                        
                        <div class="sentence-box">
                            <div class="sentence-title">üáÆüáπ Output (Italiano)</div>
                            <div id="targetTokens">
                                <div class="word-token" style="opacity: 0.3;">&lt;START&gt;</div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="progress-bar">
                        <div class="progress-fill" id="progressBar">0%</div>
                    </div>
                    
                    <div id="translationSteps"></div>
                    
                    <div style="text-align: center; margin-top: 20px;">
                        <button class="btn btn-primary" onclick="startTranslation()">‚ñ∂Ô∏è Avvia Traduzione</button>
                        <button class="btn btn-secondary" onclick="showTranslationDemo()">üîÑ Reset Demo</button>
                    </div>
                </div>
            `;
        }

        async function startTranslation() {
            const targetTokens = document.getElementById('targetTokens');
            const stepsDiv = document.getElementById('translationSteps');
            const progressBar = document.getElementById('progressBar');
            
            const translations = [
                { word: 'Il', step: 'Decoder analizza &lt;START&gt; e l\'encoder output ‚Üí Predice "Il"' },
                { word: 'gatto', step: 'Decoder analizza "&lt;START&gt; Il" e l\'encoder output ‚Üí Predice "gatto"' },
                { word: 'dorme', step: 'Decoder analizza "&lt;START&gt; Il gatto" e l\'encoder output ‚Üí Predice "dorme"' }
            ];
            
            stepsDiv.innerHTML = '';
            
            for (let i = 0; i < translations.length; i++) {
                await sleep(1500);
                
                // Add token
                const token = document.createElement('div');
                token.className = 'word-token';
                token.textContent = translations[i].word;
                token.style.animation = 'tokenFloat 1s ease-in-out';
                targetTokens.appendChild(token);
                
                // Update progress
                const progress = ((i + 1) / translations.length * 100).toFixed(0);
                progressBar.style.width = progress + '%';
                progressBar.textContent = progress + '%';
                
                // Add step explanation
                const stepDiv = document.createElement('div');
                stepDiv.className = 'translation-step';
                stepDiv.style.animation = 'tokenFloat 0.5s ease-in-out';
                stepDiv.innerHTML = `
                    <strong>Passo ${i + 1}:</strong> ${translations[i].step}
                    <br>
                    <span style="opacity: 0.9; font-size: 0.95em;">
                        ‚Üí Cross Attention: Decoder guarda tutti i token dell'encoder<br>
                        ‚Üí Self Attention: Decoder guarda i token gi√† generati<br>
                        ‚Üí Feed Forward: Elabora le informazioni<br>
                        ‚Üí Linear + Softmax: Sceglie il token pi√π probabile
                    </span>
                `;
                stepsDiv.appendChild(stepDiv);
            }
            
            await sleep(1000);
            
            // Final message
            const finalDiv = document.createElement('div');
            finalDiv.className = 'translation-step';
            finalDiv.style.background = 'rgba(132, 250, 176, 0.3)';
            finalDiv.innerHTML = `
                <strong>‚úÖ Traduzione Completata!</strong><br>
                <span style="font-size: 1.1em; margin-top: 10px; display: block;">
                    "The cat sleeps" ‚Üí "Il gatto dorme"
                </span>
                <br>
                <span style="opacity: 0.9; font-size: 0.95em;">
                    Il processo √® <strong>autoregressivo</strong>: ogni token viene generato uno alla volta,
                    usando tutti i token precedenti come contesto.
                </span>
            `;
            stepsDiv.appendChild(finalDiv);
        }

        function sleep(ms) {
            return new Promise(resolve => setTimeout(resolve, ms));
        }


        // RNN vs Transformer Comparison
        function showRNNComparison() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üîÑ Confronto: RNN/LSTM vs Transformer</h2>
                <p>Vediamo le differenze fondamentali tra questi due approcci al processamento di sequenze.</p>
            `;
            
            vis.style.display = 'block';
            vis.innerHTML = `
                <div class="comparison-container">
                    <div class="comparison-box rnn">
                        <div class="comparison-title" style="color: #f093fb;">RNN/LSTM</div>
                        <h4>‚úó Problemi:</h4>
                        <ul style="line-height: 2;">
                            <li>üêå <strong>Processamento Sequenziale:</strong> I dati devono essere processati uno alla volta</li>
                            <li>üìâ <strong>Vanishing Gradient:</strong> Difficolt√† nell'apprendere dipendenze a lungo raggio</li>
                            <li>‚è∞ <strong>Lento:</strong> Non pu√≤ essere parallelizzato</li>
                            <li>üíæ <strong>Memoria Limitata:</strong> Lo stato nascosto ha capacit√† finita</li>
                        </ul>
                        
                        <div style="margin: 20px 0;">
                            <h4 style="margin-bottom: 10px;">Architettura:</h4>
                            <div style="display: flex; align-items: center; justify-content: center; gap: 10px;">
                                <div style="padding: 15px; background: #f093fb; color: white; border-radius: 8px;">h‚ÇÄ</div>
                                <div style="font-size: 1.5em;">‚Üí</div>
                                <div style="padding: 15px; background: #f093fb; color: white; border-radius: 8px;">h‚ÇÅ</div>
                                <div style="font-size: 1.5em;">‚Üí</div>
                                <div style="padding: 15px; background: #f093fb; color: white; border-radius: 8px;">h‚ÇÇ</div>
                                <div style="font-size: 1.5em;">‚Üí</div>
                                <div style="padding: 15px; background: #f093fb; color: white; border-radius: 8px;">h‚ÇÉ</div>
                            </div>
                            <p style="text-align: center; margin-top: 10px; color: #666;">Processamento Sequenziale</p>
                        </div>
                        
                        <button class="btn btn-primary" onclick="animateVanishingGradient()" style="width: 100%; margin-top: 15px;">
                            üìâ Mostra Vanishing Gradient
                        </button>
                    </div>
                    
                    <div class="comparison-box transformer">
                        <div class="comparison-title" style="color: #667eea;">Transformer</div>
                        <h4>‚úì Vantaggi:</h4>
                        <ul style="line-height: 2;">
                            <li>‚ö° <strong>Parallelizzazione:</strong> Tutti i token processati simultaneamente</li>
                            <li>üéØ <strong>Attention Globale:</strong> Ogni token pu√≤ vedere tutti gli altri</li>
                            <li>üöÄ <strong>Veloce:</strong> Efficiente su GPU/TPU</li>
                            <li>üß† <strong>No Vanishing Gradient:</strong> Connessioni dirette tra tutti i token</li>
                        </ul>
                        
                        <div style="margin: 20px 0;">
                            <h4 style="margin-bottom: 10px;">Architettura:</h4>
                            <div style="display: flex; align-items: center; justify-content: center; gap: 10px; flex-wrap: wrap;">
                                <div style="padding: 15px; background: #667eea; color: white; border-radius: 8px;">T‚ÇÄ</div>
                                <div style="padding: 15px; background: #667eea; color: white; border-radius: 8px;">T‚ÇÅ</div>
                                <div style="padding: 15px; background: #667eea; color: white; border-radius: 8px;">T‚ÇÇ</div>
                                <div style="padding: 15px; background: #667eea; color: white; border-radius: 8px;">T‚ÇÉ</div>
                            </div>
                            <div style="text-align: center; margin-top: 10px;">
                                <div style="font-size: 1.2em; margin: 10px 0;">‚¨ç Self-Attention ‚¨ç</div>
                            </div>
                            <p style="text-align: center; color: #666;">Processamento Parallelo</p>
                        </div>
                        
                        <div style="background: linear-gradient(135deg, #84fab0, #8fd3f4); padding: 15px; border-radius: 10px; color: white; text-align: center; margin-top: 15px;">
                            <strong>Risultato:</strong> 10-100x pi√π veloce per sequenze lunghe!
                        </div>
                    </div>
                </div>
                
                <div id="vanishingGradientVis" style="display: none; margin-top: 30px;">
                    <h3 style="color: #667eea; text-align: center;">Animazione: Vanishing Gradient in RNN</h3>
                    <p style="text-align: center; color: #666; margin: 15px 0;">
                        Guarda come il gradiente si riduce man mano che si propaga indietro nel tempo
                    </p>
                    <div class="vanishing-gradient-demo" id="gradientCells" style="justify-content: center; flex-wrap: wrap;"></div>
                    <div style="margin-top: 20px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
                        <h4 style="color: #667eea;">Spiegazione:</h4>
                        <p style="line-height: 1.8;">
                            Durante il backpropagation, i gradienti vengono moltiplicati attraverso molti layer. 
                            Quando questi valori sono < 1, il gradiente diventa sempre pi√π piccolo (vanishing). 
                            Questo rende difficile per la rete imparare dipendenze a lungo raggio.
                        </p>
                        <p style="line-height: 1.8; margin-top: 10px;">
                            <strong>I Transformer risolvono questo problema</strong> con le skip connections e l'attention mechanism 
                            che fornisce connessioni dirette tra tutti i token.
                        </p>
                    </div>
                </div>
                
                <div style="margin-top: 30px; background: white; padding: 25px; border-radius: 15px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <h3 style="color: #667eea; margin-bottom: 20px;">üìä Complessit√† Computazionale</h3>
                    <table style="width: 100%; border-collapse: collapse;">
                        <thead>
                            <tr style="background: #667eea; color: white;">
                                <th style="padding: 12px; text-align: left; border-radius: 8px 0 0 0;">Modello</th>
                                <th style="padding: 12px; text-align: center;">Complessit√† per Layer</th>
                                <th style="padding: 12px; text-align: center;">Operazioni Sequenziali</th>
                                <th style="padding: 12px; text-align: center; border-radius: 0 8px 0 0;">Lunghezza Massima Path</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="background: #f8f9fa;">
                                <td style="padding: 12px; font-weight: bold;">RNN</td>
                                <td style="padding: 12px; text-align: center;">O(n¬∑d¬≤)</td>
                                <td style="padding: 12px; text-align: center;">O(n)</td>
                                <td style="padding: 12px; text-align: center;">O(n)</td>
                            </tr>
                            <tr>
                                <td style="padding: 12px; font-weight: bold;">Transformer</td>
                                <td style="padding: 12px; text-align: center;">O(n¬≤¬∑d)</td>
                                <td style="padding: 12px; text-align: center;">O(1)</td>
                                <td style="padding: 12px; text-align: center;">O(1)</td>
                            </tr>
                        </tbody>
                    </table>
                    <p style="margin-top: 15px; color: #666; font-size: 0.95em;">
                        <strong>n</strong> = lunghezza sequenza, <strong>d</strong> = dimensione del modello
                    </p>
                </div>
            `;
        }

        async function animateVanishingGradient() {
            const container = document.getElementById('vanishingGradientVis');
            const cells = document.getElementById('gradientCells');
            
            container.style.display = 'block';
            cells.innerHTML = '';
            
            const numCells = 10;
            const gradients = [];
            
            // Create cells
            for (let i = 0; i < numCells; i++) {
                const cell = document.createElement('div');
                cell.className = 'gradient-cell';
                cell.textContent = '1.0';
                cell.style.background = `rgba(102, 126, 234, 1)`;
                gradients.push(1.0);
                cells.appendChild(cell);
            }
            
            await sleep(1000);
            
            // Animate gradient decay
            for (let step = 0; step < 8; step++) {
                await sleep(500);
                const cellElements = cells.children;
                
                for (let i = numCells - 1; i >= 0; i--) {
                    gradients[i] *= 0.5; // Simulate gradient decay
                    const opacity = Math.max(gradients[i], 0.05);
                    cellElements[i].style.background = `rgba(102, 126, 234, ${opacity})`;
                    cellElements[i].textContent = gradients[i].toFixed(3);
                    
                    if (gradients[i] < 0.1) {
                        cellElements[i].style.color = '#333';
                    }
                }
            }
            
            container.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
        }

        // Interactive Attention Playground
        function showAttentionPlayground() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üéÆ Playground Interattivo per Attention</h2>
                <p>Inserisci la tua frase e vedi in tempo reale come cambiano gli attention scores!</p>
            `;
            
            vis.style.display = 'block';
            vis.innerHTML = `
                <div style="background: white; padding: 30px; border-radius: 15px; box-shadow: 0 5px 15px rgba(0,0,0,0.1);">
                    <h3 style="color: #667eea; margin-bottom: 20px;">Inserisci la tua frase</h3>
                    
                    <input type="text" 
                           id="playgroundInput" 
                           class="playground-input" 
                           placeholder="Scrivi una frase... (es: Il gatto mangia il pesce)"
                           value="Il gatto mangia il pesce">
                    
                    <div class="playground-controls">
                        <button class="btn btn-primary" onclick="calculateAttention()">
                            üîç Calcola Attention
                        </button>
                        <button class="btn btn-secondary" onclick="randomizeSentence()">
                            üé≤ Frase Casuale
                        </button>
                    </div>
                    
                    <div id="attentionWeights" style="margin: 20px 0;">
                        <h4 style="color: #667eea; margin-bottom: 15px;">‚öôÔ∏è Regola i Pesi di Attention</h4>
                        <div id="weightSliders"></div>
                    </div>
                    
                    <div id="attentionResult"></div>
                </div>
            `;
            
            calculateAttention();
        }

        function calculateAttention() {
            const input = document.getElementById('playgroundInput').value;
            const tokens = input.trim().split(/\s+/).filter(t => t.length > 0);
            
            if (tokens.length === 0) {
                return;
            }
            
            const n = tokens.length;
            
            // Generate random attention scores
            const attentionScores = [];
            for (let i = 0; i < n; i++) {
                attentionScores[i] = [];
                let sum = 0;
                for (let j = 0; j < n; j++) {
                    const score = Math.random();
                    attentionScores[i][j] = score;
                    sum += score;
                }
                // Normalize
                for (let j = 0; j < n; j++) {
                    attentionScores[i][j] /= sum;
                }
            }
            
            // Store for slider adjustments
            window.currentAttention = { tokens, scores: attentionScores };
            
            displayAttentionMatrix(tokens, attentionScores);
            createWeightSliders(tokens);
        }

        function displayAttentionMatrix(tokens, scores) {
            const result = document.getElementById('attentionResult');
            const n = tokens.length;
            
            let html = `
                <h3 style="color: #667eea; text-align: center; margin: 30px 0 20px 0;">
                    Attention Heatmap
                </h3>
                <div style="text-align: center; overflow-x: auto;">
                    <div style="display: inline-block;">
                        <div style="display: flex; justify-content: center; gap: 5px; margin: 10px 0;">
                            <div style="width: 80px;"></div>
            `;
            
            tokens.forEach(token => {
                html += `<div class="word-label" style="width: 70px;">${token}</div>`;
            });
            
            html += `</div>`;
            
            for (let i = 0; i < n; i++) {
                html += `<div style="display: flex; justify-content: center; gap: 5px; align-items: center; margin: 5px 0;">`;
                html += `<div class="word-label" style="width: 80px; text-align: right;">${tokens[i]}</div>`;
                
                for (let j = 0; j < n; j++) {
                    const score = scores[i][j];
                    const intensity = score;
                    const color = `rgba(102, 126, 234, ${intensity})`;
                    const textColor = intensity > 0.5 ? 'white' : '#333';
                    
                    html += `
                        <div class="heatmap-cell" 
                             style="background: ${color}; color: ${textColor}; width: 70px; height: 70px;"
                             title="${tokens[i]} ‚Üí ${tokens[j]}: ${score.toFixed(3)}"
                             onclick="highlightConnection(${i}, ${j})">
                            ${score.toFixed(2)}
                        </div>
                    `;
                }
                
                html += `</div>`;
            }
            
            html += `
                    </div>
                </div>
                <div style="margin: 20px 0; padding: 20px; background: #f8f9fa; border-radius: 10px;">
                    <h4 style="color: #667eea;">üí° Come interpretare:</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>Righe:</strong> Token che "fa la query" (cosa sto cercando?)</li>
                        <li><strong>Colonne:</strong> Token che "offre informazione" (cosa ho da offrire?)</li>
                        <li><strong>Colori pi√π scuri:</strong> Attention pi√π forte = maggiore rilevanza</li>
                        <li><strong>Diagonale:</strong> Self-attention (token che guarda se stesso)</li>
                    </ul>
                </div>
            `;
            
            result.innerHTML = html;
        }

        function createWeightSliders(tokens) {
            const container = document.getElementById('weightSliders');
            let html = '<p style="color: #666; margin-bottom: 15px;">Modifica quanto ogni token presta attenzione agli altri:</p>';
            
            tokens.forEach((token, idx) => {
                html += `
                    <div class="weight-slider">
                        <div class="slider-label">
                            <span>Token: "${token}"</span>
                            <span id="weight-value-${idx}">1.0</span>
                        </div>
                        <input type="range" 
                               min="0" 
                               max="200" 
                               value="100" 
                               style="width: 100%;"
                               oninput="updateAttentionWeight(${idx}, this.value)">
                    </div>
                `;
            });
            
            container.innerHTML = html;
        }

        function updateAttentionWeight(tokenIdx, value) {
            const weight = value / 100;
            document.getElementById(`weight-value-${tokenIdx}`).textContent = weight.toFixed(2);
            
            if (window.currentAttention) {
                const { tokens, scores } = window.currentAttention;
                const n = tokens.length;
                
                // Apply weight to all attention scores for this token
                const newScores = scores.map((row, i) => {
                    if (i === tokenIdx) {
                        const weightedRow = row.map(s => s * weight);
                        const sum = weightedRow.reduce((a, b) => a + b, 0);
                        return weightedRow.map(s => s / sum);
                    }
                    return row;
                });
                
                window.currentAttention.scores = newScores;
                displayAttentionMatrix(tokens, newScores);
            }
        }

        function highlightConnection(i, j) {
            const { tokens } = window.currentAttention;
            alert(`Attenzione: "${tokens[i]}" ‚Üí "${tokens[j]}"\n\nIl token "${tokens[i]}" sta prestando attenzione al token "${tokens[j]}".`);
        }

        function randomizeSentence() {
            const sentences = [
                "Il gatto mangia il pesce",
                "La macchina rossa corre veloce",
                "I bambini giocano nel parco",
                "Il sole splende nel cielo blu",
                "Maria legge un libro interessante",
                "Il cane abbaia alla luna",
                "Gli uccelli volano verso sud"
            ];
            
            const randomSentence = sentences[Math.floor(Math.random() * sentences.length)];
            document.getElementById('playgroundInput').value = randomSentence;
            calculateAttention();
        }

        // Multi-Head Attention Detail
        function showMultiHeadDetail() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üéØ Multi-Head Attention in Dettaglio</h2>
                <p>Esplora le 8 teste di attention separatamente e scopri cosa cattura ognuna!</p>
            `;
            
            vis.style.display = 'block';
            vis.innerHTML = `
                <div style="background: white; padding: 30px; border-radius: 15px; box-shadow: 0 5px 15px rgba(0,0,0,0.1);">
                    <h3 style="color: #667eea; text-align: center; margin-bottom: 25px;">
                        Le 8 Teste di Attention
                    </h3>
                    
                    <div class="multi-head-grid" id="headGrid"></div>
                    
                    <div id="headDetail" style="margin-top: 30px;"></div>
                </div>
            `;
            
            createMultiHeadGrid();
        }

        function createMultiHeadGrid() {
            const heads = [
                {
                    id: 1,
                    name: "Head 1: Sintassi",
                    description: "Cattura relazioni sintattiche come soggetto-verbo",
                    color: "#667eea",
                    focus: "grammaticali"
                },
                {
                    id: 2,
                    name: "Head 2: Semantica",
                    description: "Identifica relazioni semantiche tra concetti",
                    color: "#f093fb",
                    focus: "semantiche"
                },
                {
                    id: 3,
                    name: "Head 3: Posizione",
                    description: "Rileva pattern posizionali e distanze",
                    color: "#84fab0",
                    focus: "posizionali"
                },
                {
                    id: 4,
                    name: "Head 4: Dipendenze",
                    description: "Cattura dipendenze a lungo raggio",
                    color: "#ffd89b",
                    focus: "lunghe dipendenze"
                },
                {
                    id: 5,
                    name: "Head 5: Entit√†",
                    description: "Focalizza su nomi propri e entit√†",
                    color: "#a1c4fd",
                    focus: "entit√†"
                },
                {
                    id: 6,
                    name: "Head 6: Modificatori",
                    description: "Collega aggettivi e avverbi ai nomi/verbi",
                    color: "#fbc2eb",
                    focus: "modificatori"
                },
                {
                    id: 7,
                    name: "Head 7: Coreference",
                    description: "Risolve riferimenti pronominali",
                    color: "#ff9a9e",
                    focus: "riferimenti"
                },
                {
                    id: 8,
                    name: "Head 8: Contestuale",
                    description: "Cattura informazioni contestuali generali",
                    color: "#8fd3f4",
                    focus: "contesto"
                }
            ];
            
            const grid = document.getElementById('headGrid');
            grid.innerHTML = heads.map(head => `
                <div class="head-card" onclick="showHeadDetail(${head.id})" data-head="${head.id}">
                    <div class="head-title">${head.name}</div>
                    <div style="width: 60px; height: 60px; background: ${head.color}; border-radius: 50%; margin: 15px auto; display: flex; align-items: center; justify-content: center; color: white; font-weight: bold; font-size: 1.5em;">
                        H${head.id}
                    </div>
                    <div class="head-description">${head.description}</div>
                </div>
            `).join('');
            
            window.headData = heads;
        }

        function showHeadDetail(headId) {
            const head = window.headData.find(h => h.id === headId);
            const detailDiv = document.getElementById('headDetail');
            
            // Update selected state
            document.querySelectorAll('.head-card').forEach(card => {
                card.classList.remove('selected');
            });
            document.querySelector(`[data-head="${headId}"]`).classList.add('selected');
            
            // Generate example attention pattern for this head
            const exampleSentence = "Il gatto nero dorme sul divano";
            const tokens = exampleSentence.split(' ');
            const n = tokens.length;
            
            // Create attention pattern based on head type
            const attentionPattern = generateHeadPattern(headId, tokens);
            
            let html = `
                <div style="background: linear-gradient(135deg, ${head.color}20, ${head.color}05); padding: 25px; border-radius: 12px; border: 2px solid ${head.color};">
                    <h3 style="color: ${head.color}; margin-bottom: 15px;">
                        ${head.name}
                    </h3>
                    <p style="line-height: 1.8; margin-bottom: 20px;">
                        <strong>Specializzazione:</strong> ${head.description}
                    </p>
                    
                    <h4 style="color: ${head.color}; margin: 20px 0 15px 0;">
                        Esempio: "${exampleSentence}"
                    </h4>
                    
                    <div style="text-align: center; overflow-x: auto;">
                        <div style="display: inline-block;">
                            <div style="display: flex; gap: 3px; margin: 10px 0; justify-content: center;">
                                <div style="width: 80px;"></div>
            `;
            
            tokens.forEach(token => {
                html += `<div class="word-label" style="width: 60px; font-size: 0.85em;">${token}</div>`;
            });
            
            html += `</div>`;
            
            for (let i = 0; i < n; i++) {
                html += `<div style="display: flex; gap: 3px; margin: 3px 0; align-items: center;">`;
                html += `<div class="word-label" style="width: 80px; text-align: right; font-size: 0.85em;">${tokens[i]}</div>`;
                
                for (let j = 0; j < n; j++) {
                    const score = attentionPattern[i][j];
                    const intensity = score;
                    const bgColor = `rgba(${parseInt(head.color.slice(1,3), 16)}, ${parseInt(head.color.slice(3,5), 16)}, ${parseInt(head.color.slice(5,7), 16)}, ${intensity})`;
                    const textColor = intensity > 0.5 ? 'white' : '#333';
                    
                    html += `
                        <div style="width: 60px; height: 60px; background: ${bgColor}; color: ${textColor}; 
                                    display: flex; align-items: center; justify-content: center; 
                                    border-radius: 6px; font-size: 0.8em; font-weight: bold; cursor: pointer;
                                    border: 1px solid #ddd;"
                             title="${tokens[i]} ‚Üí ${tokens[j]}: ${score.toFixed(3)}">
                            ${score.toFixed(2)}
                        </div>
                    `;
                }
                
                html += `</div>`;
            }
            
            html += `
                        </div>
                    </div>
                    
                    <div style="margin-top: 20px; padding: 15px; background: white; border-radius: 8px;">
                        <h4 style="color: ${head.color}; margin-bottom: 10px;">üí° Osservazioni:</h4>
                        ${getHeadObservations(headId)}
                    </div>
                </div>
            `;
            
            detailDiv.innerHTML = html;
            detailDiv.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
        }

        function generateHeadPattern(headId, tokens) {
            const n = tokens.length;
            const pattern = [];
            
            for (let i = 0; i < n; i++) {
                pattern[i] = [];
                for (let j = 0; j < n; j++) {
                    let score = 0;
                    
                    switch(headId) {
                        case 1: // Sintassi - focus on adjacent tokens
                            score = Math.abs(i - j) <= 1 ? 0.8 : 0.2;
                            break;
                        case 2: // Semantica - nouns and verbs
                            score = (i === 1 && j === 3) || (i === 3 && j === 1) ? 0.9 : 0.3;
                            break;
                        case 3: // Posizione - diagonal pattern
                            score = i === j ? 0.7 : 0.3 / (Math.abs(i - j) + 1);
                            break;
                        case 4: // Dipendenze lunghe
                            score = Math.abs(i - j) > 2 ? 0.8 : 0.2;
                            break;
                        case 5: // Entit√†
                            score = (i <= 2 && j <= 2) ? 0.8 : 0.2;
                            break;
                        case 6: // Modificatori
                            score = (i === 2 && j === 1) || (i === 1 && j === 2) ? 0.9 : 0.3;
                            break;
                        case 7: // Coreference
                            score = i === j ? 0.9 : 0.1;
                            break;
                        case 8: // Contestuale - uniform
                            score = 0.5 + Math.random() * 0.3;
                            break;
                    }
                    
                    pattern[i][j] = score;
                }
                
                // Normalize
                const sum = pattern[i].reduce((a, b) => a + b, 0);
                pattern[i] = pattern[i].map(s => s / sum);
            }
            
            return pattern;
        }

        function getHeadObservations(headId) {
            const observations = {
                1: `<ul style="line-height: 1.8;">
                        <li>Focus su token adiacenti (sintassi locale)</li>
                        <li>Alta attenzione tra soggetto "gatto" e verbo "dorme"</li>
                        <li>Pattern tipico per relazioni grammaticali</li>
                    </ul>`,
                2: `<ul style="line-height: 1.8;">
                        <li>Connette "gatto" con "dorme" (soggetto-azione)</li>
                        <li>Relazioni semantiche oltre la vicinanza sintattica</li>
                        <li>Capisce il significato oltre alla struttura</li>
                    </ul>`,
                3: `<ul style="line-height: 1.8;">
                        <li>Self-attention forte (diagonale)</li>
                        <li>Pattern che decrescono con la distanza</li>
                        <li>Codifica informazioni posizionali</li>
                    </ul>`,
                4: `<ul style="line-height: 1.8;">
                        <li>Focus su dipendenze a lungo raggio</li>
                        <li>Collega inizio e fine della frase</li>
                        <li>Ignora token vicini</li>
                    </ul>`,
                5: `<ul style="line-height: 1.8;">
                        <li>Alta attenzione su "Il gatto nero" (entit√† completa)</li>
                        <li>Riconosce sintagmi nominali</li>
                        <li>Raggruppa elementi correlati</li>
                    </ul>`,
                6: `<ul style="line-height: 1.8;">
                        <li>Collega "nero" a "gatto" (aggettivo-nome)</li>
                        <li>Pattern tipico per modificatori</li>
                        <li>Relazioni di tipo attribute-value</li>
                    </ul>`,
                7: `<ul style="line-height: 1.8;">
                        <li>Self-attention molto forte</li>
                        <li>Mantiene identit√† dei token</li>
                        <li>Importante per risolvere pronomi</li>
                    </ul>`,
                8: `<ul style="line-height: 1.8;">
                        <li>Attention distribuita uniformemente</li>
                        <li>Cattura contesto globale</li>
                        <li>Backup generale per informazioni mancate da altre teste</li>
                    </ul>`
            };
            
            return observations[headId] || '';
        }


        // Interactive Positional Encoding
        function showPositionalEncodingInteractive() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üìä Positional Encoding 3D Interattivo</h2>
                <p>Esplora la visualizzazione 3D delle sinusoidi e come catturano le posizioni!</p>
            `;
            
            vis.style.display = 'block';
            vis.innerHTML = `
                <div style="background: white; padding: 30px; border-radius: 15px; box-shadow: 0 5px 15px rgba(0,0,0,0.1);">
                    <h3 style="color: #667eea; text-align: center; margin-bottom: 25px;">
                        Visualizzazione Positional Encoding
                    </h3>
                    
                    <div style="margin: 20px 0;">
                        <div class="slider-label">
                            <span>Lunghezza Sequenza: <span id="seqLenValue">10</span></span>
                        </div>
                        <input type="range" min="5" max="50" value="10" style="width: 100%;" 
                               oninput="updatePositionalEncoding('seqLen', this.value)">
                    </div>
                    
                    <div style="margin: 20px 0;">
                        <div class="slider-label">
                            <span>Dimensione Embedding: <span id="dimValue">64</span></span>
                        </div>
                        <input type="range" min="16" max="128" step="16" value="64" style="width: 100%;" 
                               oninput="updatePositionalEncoding('dim', this.value)">
                    </div>
                    
                    <div style="margin: 20px 0;">
                        <div class="slider-label">
                            <span>Frequenza Base: <span id="freqValue">10000</span></span>
                        </div>
                        <input type="range" min="1000" max="20000" step="1000" value="10000" style="width: 100%;" 
                               oninput="updatePositionalEncoding('freq', this.value)">
                    </div>
                    
                    <canvas id="posEncodingCanvas" class="canvas-3d" width="800" height="500"></canvas>
                    
                    <div style="margin-top: 20px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
                        <h4 style="color: #667eea; margin-bottom: 15px;">üìê Formula:</h4>
                        <div class="code-example">
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
                        </div>
                        <h4 style="color: #667eea; margin: 15px 0 10px 0;">üí° Caratteristiche:</h4>
                        <ul style="line-height: 1.8;">
                            <li><strong>Periodico:</strong> Usa sin/cos per creare pattern ripetuti</li>
                            <li><strong>Deterministico:</strong> Stessa posizione = stesso encoding sempre</li>
                            <li><strong>Relativo:</strong> Facile calcolare distanze relative</li>
                            <li><strong>Infinito:</strong> Pu√≤ estendersi a sequenze di qualsiasi lunghezza</li>
                            <li><strong>Frequenze diverse:</strong> Dimensioni diverse = frequenze diverse</li>
                        </ul>
                    </div>
                </div>
            `;
            
            drawPositionalEncoding(10, 64, 10000);
        }

        function updatePositionalEncoding(param, value) {
            document.getElementById(param + 'Value').textContent = value;
            
            const seqLen = parseInt(document.getElementById('seqLenValue').textContent);
            const dim = parseInt(document.getElementById('dimValue').textContent);
            const freq = parseInt(document.getElementById('freqValue').textContent);
            
            drawPositionalEncoding(seqLen, dim, freq);
        }

        function drawPositionalEncoding(seqLen, dim, freqBase) {
            const canvas = document.getElementById('posEncodingCanvas');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;
            
            // Clear canvas
            ctx.fillStyle = '#f8f9fa';
            ctx.fillRect(0, 0, width, height);
            
            // Calculate positional encodings
            const encodings = [];
            for (let pos = 0; pos < seqLen; pos++) {
                encodings[pos] = [];
                for (let i = 0; i < dim; i++) {
                    const angle = pos / Math.pow(freqBase, (2 * Math.floor(i/2)) / dim);
                    encodings[pos][i] = i % 2 === 0 ? Math.sin(angle) : Math.cos(angle);
                }
            }
            
            // Draw heatmap
            const cellWidth = Math.min(width / dim, 50);
            const cellHeight = Math.min(height / seqLen, 50);
            const startX = (width - cellWidth * dim) / 2;
            const startY = 50;
            
            // Draw title
            ctx.fillStyle = '#667eea';
            ctx.font = 'bold 16px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Positional Encoding Heatmap', width/2, 30);
            
            // Draw cells
            for (let pos = 0; pos < seqLen; pos++) {
                for (let i = 0; i < dim; i++) {
                    const value = encodings[pos][i];
                    const normalized = (value + 1) / 2; // Normalize to 0-1
                    
                    // Color gradient from blue to red
                    const r = Math.floor(234 * normalized + 102 * (1 - normalized));
                    const g = Math.floor(126 * (1 - normalized) + 126 * normalized);
                    const b = Math.floor(234 * (1 - normalized) + 234 * normalized);
                    
                    ctx.fillStyle = `rgb(${r}, ${g}, ${b})`;
                    ctx.fillRect(
                        startX + i * cellWidth,
                        startY + pos * cellHeight,
                        cellWidth - 1,
                        cellHeight - 1
                    );
                }
            }
            
            // Draw axes labels
            ctx.fillStyle = '#333';
            ctx.font = '12px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Position ‚Üí', startX, height - 10);
            ctx.save();
            ctx.translate(10, startY + (seqLen * cellHeight) / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('Dimension ‚Üí', 0, 0);
            ctx.restore();
        }

        // Training Visualization
        function showTrainingVisualization() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üìà Visualizzazione del Training</h2>
                <p>Guarda come il modello impara durante il training con loss curves e evoluzione dei pesi!</p>
            `;
            
            vis.style.display = 'block';
            vis.innerHTML = `
                <div style="background: white; padding: 30px; border-radius: 15px; box-shadow: 0 5px 15px rgba(0,0,0,0.1);">
                    <h3 style="color: #667eea; text-align: center; margin-bottom: 25px;">
                        Training Progress
                    </h3>
                    
                    <div style="text-align: center; margin: 20px 0;">
                        <button class="btn btn-primary" onclick="startTraining()">
                            ‚ñ∂Ô∏è Avvia Training
                        </button>
                        <button class="btn btn-secondary" onclick="pauseTraining()">
                            ‚è∏Ô∏è Pausa
                        </button>
                        <button class="btn btn-secondary" onclick="resetTraining()">
                            üîÑ Reset
                        </button>
                    </div>
                    
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
                        <div>
                            <h4 style="color: #667eea; margin-bottom: 10px;">üìâ Loss Curve</h4>
                            <canvas id="lossCanvas" width="400" height="300" style="border: 2px solid #e0e0e0; border-radius: 8px; background: white;"></canvas>
                        </div>
                        <div>
                            <h4 style="color: #667eea; margin-bottom: 10px;">üìä Accuracy</h4>
                            <canvas id="accuracyCanvas" width="400" height="300" style="border: 2px solid #e0e0e0; border-radius: 8px; background: white;"></canvas>
                        </div>
                    </div>
                    
                    <div id="trainingStats" style="background: #f8f9fa; padding: 20px; border-radius: 10px; margin-top: 20px;">
                        <h4 style="color: #667eea; margin-bottom: 15px;">üìä Training Statistics</h4>
                        <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px;">
                            <div style="text-align: center;">
                                <div style="font-size: 2em; font-weight: bold; color: #667eea;" id="epochCount">0</div>
                                <div style="color: #666;">Epoch</div>
                            </div>
                            <div style="text-align: center;">
                                <div style="font-size: 2em; font-weight: bold; color: #f093fb;" id="lossValue">-</div>
                                <div style="color: #666;">Loss</div>
                            </div>
                            <div style="text-align: center;">
                                <div style="font-size: 2em; font-weight: bold; color: #84fab0;" id="accValue">-</div>
                                <div style="color: #666;">Accuracy</div>
                            </div>
                            <div style="text-align: center;">
                                <div style="font-size: 2em; font-weight: bold; color: #ffd89b;" id="timeValue">0s</div>
                                <div style="color: #666;">Time</div>
                            </div>
                        </div>
                    </div>
                    
                    <div style="margin-top: 20px;">
                        <h4 style="color: #667eea; margin-bottom: 15px;">üéØ Evoluzione dei Pesi</h4>
                        <canvas id="weightsCanvas" width="800" height="200" style="border: 2px solid #e0e0e0; border-radius: 8px; background: white;"></canvas>
                    </div>
                </div>
            `;
            
            initializeTraining();
        }

        let trainingState = {
            epoch: 0,
            losses: [],
            accuracies: [],
            weights: [],
            isTraining: false,
            startTime: null
        };

        function initializeTraining() {
            trainingState = {
                epoch: 0,
                losses: [],
                accuracies: [],
                weights: [],
                isTraining: false,
                startTime: null
            };
            
            // Initialize weights
            for (let i = 0; i < 50; i++) {
                trainingState.weights.push((Math.random() - 0.5) * 2);
            }
            
            updateTrainingCharts();
        }

        async function startTraining() {
            if (trainingState.isTraining) return;
            
            trainingState.isTraining = true;
            trainingState.startTime = trainingState.startTime || Date.now();
            
            while (trainingState.isTraining && trainingState.epoch < 100) {
                trainingState.epoch++;
                
                // Simulate training loss decrease
                const loss = 2.5 * Math.exp(-0.05 * trainingState.epoch) + 0.1 + Math.random() * 0.1;
                const accuracy = Math.min(0.95, 1 - Math.exp(-0.08 * trainingState.epoch)) + Math.random() * 0.02;
                
                trainingState.losses.push(loss);
                trainingState.accuracies.push(accuracy);
                
                // Update weights slightly
                trainingState.weights = trainingState.weights.map(w => {
                    return w + (Math.random() - 0.5) * 0.1 * Math.exp(-0.03 * trainingState.epoch);
                });
                
                updateTrainingCharts();
                updateTrainingStats();
                
                await sleep(100);
            }
            
            trainingState.isTraining = false;
        }

        function pauseTraining() {
            trainingState.isTraining = false;
        }

        function resetTraining() {
            pauseTraining();
            initializeTraining();
            updateTrainingStats();
        }

        function updateTrainingCharts() {
            drawLossCurve();
            drawAccuracyCurve();
            drawWeights();
        }

        function updateTrainingStats() {
            document.getElementById('epochCount').textContent = trainingState.epoch;
            
            if (trainingState.losses.length > 0) {
                const lastLoss = trainingState.losses[trainingState.losses.length - 1];
                document.getElementById('lossValue').textContent = lastLoss.toFixed(3);
                
                const lastAcc = trainingState.accuracies[trainingState.accuracies.length - 1];
                document.getElementById('accValue').textContent = (lastAcc * 100).toFixed(1) + '%';
                
                const elapsed = Math.floor((Date.now() - trainingState.startTime) / 1000);
                document.getElementById('timeValue').textContent = elapsed + 's';
            }
        }

        function drawLossCurve() {
            const canvas = document.getElementById('lossCanvas');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;
            
            ctx.clearRect(0, 0, width, height);
            
            if (trainingState.losses.length < 2) return;
            
            // Draw axes
            ctx.strokeStyle = '#ccc';
            ctx.lineWidth = 1;
            ctx.beginPath();
            ctx.moveTo(40, 20);
            ctx.lineTo(40, height - 40);
            ctx.lineTo(width - 20, height - 40);
            ctx.stroke();
            
            // Draw labels
            ctx.fillStyle = '#666';
            ctx.font = '12px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Epoch', width / 2, height - 10);
            ctx.save();
            ctx.translate(15, height / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('Loss', 0, 0);
            ctx.restore();
            
            // Draw curve
            const maxLoss = Math.max(...trainingState.losses);
            const xScale = (width - 60) / Math.max(trainingState.losses.length, 1);
            const yScale = (height - 60) / maxLoss;
            
            ctx.strokeStyle = '#667eea';
            ctx.lineWidth = 2;
            ctx.beginPath();
            
            trainingState.losses.forEach((loss, i) => {
                const x = 40 + i * xScale;
                const y = height - 40 - loss * yScale;
                
                if (i === 0) {
                    ctx.moveTo(x, y);
                } else {
                    ctx.lineTo(x, y);
                }
            });
            
            ctx.stroke();
            
            // Draw points
            ctx.fillStyle = '#667eea';
            trainingState.losses.forEach((loss, i) => {
                const x = 40 + i * xScale;
                const y = height - 40 - loss * yScale;
                ctx.beginPath();
                ctx.arc(x, y, 3, 0, Math.PI * 2);
                ctx.fill();
            });
        }

        function drawAccuracyCurve() {
            const canvas = document.getElementById('accuracyCanvas');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;
            
            ctx.clearRect(0, 0, width, height);
            
            if (trainingState.accuracies.length < 2) return;
            
            // Draw axes
            ctx.strokeStyle = '#ccc';
            ctx.lineWidth = 1;
            ctx.beginPath();
            ctx.moveTo(40, 20);
            ctx.lineTo(40, height - 40);
            ctx.lineTo(width - 20, height - 40);
            ctx.stroke();
            
            // Draw labels
            ctx.fillStyle = '#666';
            ctx.font = '12px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Epoch', width / 2, height - 10);
            ctx.save();
            ctx.translate(15, height / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('Accuracy', 0, 0);
            ctx.restore();
            
            // Draw curve
            const xScale = (width - 60) / Math.max(trainingState.accuracies.length, 1);
            const yScale = (height - 60);
            
            ctx.strokeStyle = '#84fab0';
            ctx.lineWidth = 2;
            ctx.beginPath();
            
            trainingState.accuracies.forEach((acc, i) => {
                const x = 40 + i * xScale;
                const y = height - 40 - acc * yScale;
                
                if (i === 0) {
                    ctx.moveTo(x, y);
                } else {
                    ctx.lineTo(x, y);
                }
            });
            
            ctx.stroke();
            
            // Draw points
            ctx.fillStyle = '#84fab0';
            trainingState.accuracies.forEach((acc, i) => {
                const x = 40 + i * xScale;
                const y = height - 40 - acc * yScale;
                ctx.beginPath();
                ctx.arc(x, y, 3, 0, Math.PI * 2);
                ctx.fill();
            });
        }

        function drawWeights() {
            const canvas = document.getElementById('weightsCanvas');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;
            
            ctx.clearRect(0, 0, width, height);
            
            const barWidth = width / trainingState.weights.length;
            const maxWeight = Math.max(...trainingState.weights.map(Math.abs), 1);
            
            trainingState.weights.forEach((weight, i) => {
                const barHeight = (Math.abs(weight) / maxWeight) * (height / 2 - 20);
                const x = i * barWidth;
                const y = weight >= 0 ? height / 2 - barHeight : height / 2;
                
                ctx.fillStyle = weight >= 0 ? '#667eea' : '#f093fb';
                ctx.fillRect(x, y, barWidth - 1, Math.abs(barHeight));
            });
            
            // Draw center line
            ctx.strokeStyle = '#333';
            ctx.lineWidth = 1;
            ctx.beginPath();
            ctx.moveTo(0, height / 2);
            ctx.lineTo(width, height / 2);
            ctx.stroke();
        }


        // Complexity Calculator
        function showComplexityCalculator() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üßÆ Calculatore di Complessit√†</h2>
                <p>Calcola FLOPS, memoria richiesta e tempo di inferenza per il tuo modello Transformer!</p>
            `;
            
            vis.style.display = 'block';
            vis.innerHTML = `
                <div style="background: white; padding: 30px; border-radius: 15px; box-shadow: 0 5px 15px rgba(0,0,0,0.1);">
                    <h3 style="color: #667eea; text-align: center; margin-bottom: 25px;">
                        Configurazione Modello
                    </h3>
                    
                    <div class="calculator-grid">
                        <div>
                            <div class="calculator-input-group">
                                <label class="calculator-label">Lunghezza Sequenza (n)</label>
                                <input type="number" id="seqLength" class="calculator-input" value="512" min="1" max="10000">
                            </div>
                            
                            <div class="calculator-input-group">
                                <label class="calculator-label">Dimensione Modello (d_model)</label>
                                <input type="number" id="modelDim" class="calculator-input" value="512" min="64" max="2048" step="64">
                            </div>
                            
                            <div class="calculator-input-group">
                                <label class="calculator-label">Numero di Heads</label>
                                <input type="number" id="numHeads" class="calculator-input" value="8" min="1" max="32">
                            </div>
                            
                            <div class="calculator-input-group">
                                <label class="calculator-label">Numero di Layers</label>
                                <input type="number" id="numLayers" class="calculator-input" value="6" min="1" max="96">
                            </div>
                        </div>
                        
                        <div>
                            <div class="calculator-input-group">
                                <label class="calculator-label">Dimensione FFN</label>
                                <input type="number" id="ffnDim" class="calculator-input" value="2048" min="256" max="8192" step="256">
                            </div>
                            
                            <div class="calculator-input-group">
                                <label class="calculator-label">Dimensione Vocabolario</label>
                                <input type="number" id="vocabSize" class="calculator-input" value="50000" min="1000" max="500000" step="1000">
                            </div>
                            
                            <div class="calculator-input-group">
                                <label class="calculator-label">Batch Size</label>
                                <input type="number" id="batchSize" class="calculator-input" value="32" min="1" max="512">
                            </div>
                            
                            <div class="calculator-input-group">
                                <label class="calculator-label">Hardware (TFLOPS)</label>
                                <input type="number" id="hardwareTflops" class="calculator-input" value="100" min="1" max="1000" step="10">
                                <small style="color: #666;">V100=125, A100=312, H100=1000</small>
                            </div>
                        </div>
                    </div>
                    
                    <div style="text-align: center; margin: 25px 0;">
                        <button class="btn btn-primary" onclick="calculateComplexity()" style="font-size: 1.1em; padding: 15px 40px;">
                            üßÆ Calcola
                        </button>
                    </div>
                    
                    <div id="complexityResults"></div>
                </div>
            `;
        }

        function calculateComplexity() {
            const n = parseInt(document.getElementById('seqLength').value);
            const d = parseInt(document.getElementById('modelDim').value);
            const h = parseInt(document.getElementById('numHeads').value);
            const L = parseInt(document.getElementById('numLayers').value);
            const dff = parseInt(document.getElementById('ffnDim').value);
            const V = parseInt(document.getElementById('vocabSize').value);
            const B = parseInt(document.getElementById('batchSize').value);
            const tflops = parseInt(document.getElementById('hardwareTflops').value);
            
            // Calculate FLOPs per layer
            const attentionFlops = 4 * B * n * d * d; // QKV projections + output projection
            const attentionScoreFlops = 2 * B * h * n * n * (d / h); // QK^T and softmax * V
            const ffnFlops = 2 * B * n * d * dff * 2; // Two linear layers
            
            const flopsPerLayer = attentionFlops + attentionScoreFlops + ffnFlops;
            const totalFlops = flopsPerLayer * L;
            
            // Memory (parameters)
            const embeddingParams = V * d;
            const attentionParams = L * (4 * d * d); // QKV + output
            const ffnParams = L * (2 * d * dff); // Two layers
            const totalParams = embeddingParams + attentionParams + ffnParams;
            
            // Memory in GB (fp32 = 4 bytes)
            const paramMemoryGB = (totalParams * 4) / (1024 ** 3);
            
            // Activation memory (approximate)
            const activationMemoryGB = (B * n * d * L * 10 * 4) / (1024 ** 3);
            
            // Inference time
            const inferenceTimeMs = (totalFlops / (tflops * 1e12)) * 1000;
            
            // Display results
            const resultsDiv = document.getElementById('complexityResults');
            resultsDiv.innerHTML = `
                <div class="calculator-result">
                    <h3 style="margin-bottom: 20px; text-align: center;">üìä Risultati</h3>
                    
                    <div class="result-item">
                        <span class="result-label">FLOPs per Forward Pass:</span>
                        <span class="result-value">${formatNumber(totalFlops)}</span>
                    </div>
                    
                    <div class="result-item">
                        <span class="result-label">Parametri Totali:</span>
                        <span class="result-value">${formatNumber(totalParams)} (${(totalParams / 1e6).toFixed(1)}M)</span>
                    </div>
                    
                    <div class="result-item">
                        <span class="result-label">Memoria Parametri:</span>
                        <span class="result-value">${paramMemoryGB.toFixed(2)} GB</span>
                    </div>
                    
                    <div class="result-item">
                        <span class="result-label">Memoria Activations:</span>
                        <span class="result-value">${activationMemoryGB.toFixed(2)} GB</span>
                    </div>
                    
                    <div class="result-item">
                        <span class="result-label">Memoria Totale:</span>
                        <span class="result-value">${(paramMemoryGB + activationMemoryGB).toFixed(2)} GB</span>
                    </div>
                    
                    <div class="result-item">
                        <span class="result-label">Tempo Inferenza (stimato):</span>
                        <span class="result-value">${inferenceTimeMs.toFixed(2)} ms</span>
                    </div>
                    
                    <div class="result-item">
                        <span class="result-label">Throughput:</span>
                        <span class="result-value">${(1000 / inferenceTimeMs * B).toFixed(1)} samples/sec</span>
                    </div>
                </div>
                
                <div style="margin-top: 20px; padding: 20px; background: white; border-radius: 10px;">
                    <h4 style="color: #667eea; margin-bottom: 15px;">üìà Breakdown per Component</h4>
                    <canvas id="complexityChart" width="700" height="300" style="display: block; margin: 0 auto;"></canvas>
                </div>
                
                <div style="margin-top: 20px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
                    <h4 style="color: #667eea; margin-bottom: 15px;">üí° Note:</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>Self-Attention:</strong> Complessit√† O(n¬≤¬∑d) - costoso per sequenze lunghe</li>
                        <li><strong>Feed-Forward:</strong> Complessit√† O(n¬∑d¬≤) - dominante per sequenze corte</li>
                        <li><strong>Embedding:</strong> Memoria proporzionale a V¬∑d (vocabolario √ó dimensione)</li>
                        <li><strong>Training:</strong> Richiede 3-4x pi√π memoria per gradienti e optimizer states</li>
                    </ul>
                </div>
            `;
            
            drawComplexityChart(attentionFlops * L, ffnFlops * L, embeddingParams, attentionParams, ffnParams);
        }

        function formatNumber(num) {
            if (num >= 1e12) return (num / 1e12).toFixed(2) + 'T';
            if (num >= 1e9) return (num / 1e9).toFixed(2) + 'G';
            if (num >= 1e6) return (num / 1e6).toFixed(2) + 'M';
            if (num >= 1e3) return (num / 1e3).toFixed(2) + 'K';
            return num.toFixed(0);
        }

        function drawComplexityChart(attFlops, ffnFlops, embParams, attParams, ffnParams) {
            const canvas = document.getElementById('complexityChart');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;
            
            ctx.clearRect(0, 0, width, height);
            
            // Draw FLOPs breakdown
            const flopsData = [
                { label: 'Attention', value: attFlops, color: '#667eea' },
                { label: 'Feed-Forward', value: ffnFlops, color: '#f093fb' }
            ];
            
            const paramsData = [
                { label: 'Embedding', value: embParams, color: '#84fab0' },
                { label: 'Attention', value: attParams, color: '#667eea' },
                { label: 'Feed-Forward', value: ffnParams, color: '#f093fb' }
            ];
            
            // Draw pie charts
            drawPieChart(ctx, flopsData, 175, height/2, 100, 'FLOPs Distribution');
            drawPieChart(ctx, paramsData, 525, height/2, 100, 'Parameters Distribution');
        }

        function drawPieChart(ctx, data, centerX, centerY, radius, title) {
            const total = data.reduce((sum, item) => sum + item.value, 0);
            
            // Draw title
            ctx.fillStyle = '#333';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText(title, centerX, centerY - radius - 20);
            
            let startAngle = -Math.PI / 2;
            
            data.forEach((item, i) => {
                const sliceAngle = (item.value / total) * 2 * Math.PI;
                
                // Draw slice
                ctx.fillStyle = item.color;
                ctx.beginPath();
                ctx.moveTo(centerX, centerY);
                ctx.arc(centerX, centerY, radius, startAngle, startAngle + sliceAngle);
                ctx.closePath();
                ctx.fill();
                
                // Draw label
                const labelAngle = startAngle + sliceAngle / 2;
                const labelX = centerX + Math.cos(labelAngle) * (radius + 30);
                const labelY = centerY + Math.sin(labelAngle) * (radius + 30);
                
                ctx.fillStyle = item.color;
                ctx.font = '12px Arial';
                ctx.textAlign = labelX > centerX ? 'left' : 'right';
                ctx.fillText(`${item.label}`, labelX, labelY);
                ctx.fillText(`${((item.value / total) * 100).toFixed(1)}%`, labelX, labelY + 15);
                
                startAngle += sliceAngle;
            });
        }

        // Quiz Mode
        function showQuizMode() {
            const vis = document.getElementById('visualization');
            const panel = document.getElementById('infoPanel');
            
            panel.innerHTML = `
                <h2>üéì Quiz: Test la tua Comprensione</h2>
                <p>Rispondi alle domande per verificare la tua conoscenza dei Transformer!</p>
            `;
            
            vis.style.display = 'block';
            
            window.quizState = {
                currentQuestion: 0,
                score: 0,
                answered: []
            };
            
            showQuizQuestion();
        }

        const quizQuestions = [
            {
                question: "Qual √® la complessit√† computazionale del self-attention?",
                options: ["O(n)", "O(n¬∑d)", "O(n¬≤¬∑d)", "O(n¬∑d¬≤)"],
                correct: 2,
                explanation: "Il self-attention ha complessit√† O(n¬≤¬∑d) perch√© calcola attention scores tra tutte le coppie di token (n¬≤) per ogni dimensione (d)."
            },
            {
                question: "Quante teste di attention usa il Transformer originale?",
                options: ["4", "8", "16", "32"],
                correct: 1,
                explanation: "Il paper 'Attention is All You Need' usa 8 teste di attention in parallelo."
            },
            {
                question: "Cosa fa il Positional Encoding?",
                options: [
                    "Normalizza i vettori di embedding",
                    "Aggiunge informazione sulla posizione dei token",
                    "Aumenta la dimensione dei vettori",
                    "Applica dropout"
                ],
                correct: 1,
                explanation: "Il Positional Encoding aggiunge informazione sulla posizione dei token nella sequenza, dato che il Transformer non ha ricorrenza."
            },
            {
                question: "Quale problema delle RNN risolvono i Transformer?",
                options: [
                    "Overfitting",
                    "Vanishing gradient",
                    "Underfitting",
                    "Exploding activation"
                ],
                correct: 1,
                explanation: "I Transformer risolvono il problema del vanishing gradient grazie alle skip connections e all'attention che fornisce connessioni dirette tra tutti i token."
            },
            {
                question: "Cosa significa 'masked' attention nel decoder?",
                options: [
                    "Alcuni token vengono rimossi",
                    "L'attenzione viene normalizzata",
                    "Si previene di guardare i token futuri",
                    "I pesi vengono azzerati"
                ],
                correct: 2,
                explanation: "La masked attention previene al decoder di 'sbirciare' i token futuri durante il training, mantenendo l'auto-regressivit√†."
            }
        ];

        function showQuizQuestion() {
            const vis = document.getElementById('visualization');
            const q = window.quizState;
            const question = quizQuestions[q.currentQuestion];
            
            vis.innerHTML = `
                <div class="quiz-container">
                    <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 20px;">
                        <div style="color: #667eea; font-weight: bold;">
                            Domanda ${q.currentQuestion + 1} di ${quizQuestions.length}
                        </div>
                        <div style="color: #84fab0; font-weight: bold; font-size: 1.2em;">
                            Punteggio: ${q.score}/${q.currentQuestion}
                        </div>
                    </div>
                    
                    <div class="quiz-question">
                        ${question.question}
                    </div>
                    
                    <div class="quiz-options" id="quizOptions">
                        ${question.options.map((opt, i) => `
                            <div class="quiz-option" onclick="answerQuiz(${i})" id="option-${i}">
                                ${String.fromCharCode(65 + i)}. ${opt}
                            </div>
                        `).join('')}
                    </div>
                    
                    <div id="quizFeedback" style="margin-top: 20px;"></div>
                </div>
            `;
        }

        function answerQuiz(selected) {
            const q = window.quizState;
            if (q.answered[q.currentQuestion]) return;
            
            q.answered[q.currentQuestion] = true;
            const question = quizQuestions[q.currentQuestion];
            const correct = selected === question.correct;
            
            if (correct) {
                q.score++;
            }
            
            // Update UI
            document.getElementById(`option-${selected}`).classList.add(correct ? 'correct' : 'incorrect');
            document.getElementById(`option-${question.correct}`).classList.add('correct');
            
            // Disable all options
            document.querySelectorAll('.quiz-option').forEach(opt => {
                opt.style.pointerEvents = 'none';
            });
            
            // Show feedback
            const feedback = document.getElementById('quizFeedback');
            feedback.innerHTML = `
                <div style="background: ${correct ? '#84fab0' : '#ff9a9e'}; color: white; padding: 20px; border-radius: 10px;">
                    <h4 style="margin-bottom: 10px;">${correct ? '‚úÖ Corretto!' : '‚ùå Sbagliato!'}</h4>
                    <p style="line-height: 1.6;">${question.explanation}</p>
                </div>
                
                <div style="text-align: center; margin-top: 20px;">
                    <button class="btn btn-primary" onclick="nextQuizQuestion()">
                        ${q.currentQuestion < quizQuestions.length - 1 ? 'Prossima Domanda ‚Üí' : 'üèÅ Vedi Risultati'}
                    </button>
                </div>
            `;
        }

        function nextQuizQuestion() {
            const q = window.quizState;
            q.currentQuestion++;
            
            if (q.currentQuestion < quizQuestions.length) {
                showQuizQuestion();
            } else {
                showQuizResults();
            }
        }

        function showQuizResults() {
            const vis = document.getElementById('visualization');
            const q = window.quizState;
            const percentage = (q.score / quizQuestions.length * 100).toFixed(0);
            
            let grade, message;
            if (percentage >= 90) {
                grade = 'üèÜ Eccellente!';
                message = 'Hai una comprensione eccezionale dei Transformer!';
            } else if (percentage >= 70) {
                grade = '‚ú® Molto Bene!';
                message = 'Ottima conoscenza, continua cos√¨!';
            } else if (percentage >= 50) {
                grade = 'üëç Buono!';
                message = 'Buona base, rileggi alcune sezioni per migliorare.';
            } else {
                grade = 'üìö Da Rivedere';
                message = 'Consiglio di ripassare il tutorial prima di riprovare.';
            }
            
            vis.innerHTML = `
                <div class="quiz-container">
                    <h2 style="text-align: center; color: #667eea; margin-bottom: 20px;">
                        Quiz Completato!
                    </h2>
                    
                    <div style="background: linear-gradient(135deg, #667eea, #764ba2); color: white; padding: 40px; border-radius: 15px; text-align: center;">
                        <div style="font-size: 3em; margin-bottom: 20px;">${grade}</div>
                        <div style="font-size: 2.5em; font-weight: bold; margin-bottom: 10px;">
                            ${q.score} / ${quizQuestions.length}
                        </div>
                        <div style="font-size: 1.5em; margin-bottom: 20px;">
                            ${percentage}%
                        </div>
                        <div style="font-size: 1.1em; opacity: 0.9;">
                            ${message}
                        </div>
                    </div>
                    
                    <div style="text-align: center; margin-top: 30px; display: flex; gap: 15px; justify-content: center;">
                        <button class="btn btn-primary" onclick="showQuizMode()">
                            üîÑ Riprova
                        </button>
                        <button class="btn btn-secondary" onclick="showOverview()">
                            üìã Torna alla Panoramica
                        </button>
                    </div>
                </div>
            `;
        }

        // Code View Toggle
        let codeViewEnabled = false;

        function toggleCodeView() {
            codeViewEnabled = !codeViewEnabled;
            
            if (codeViewEnabled) {
                showCodeViewPanel();
            } else {
                const codePanel = document.getElementById('codeViewPanel');
                if (codePanel) {
                    codePanel.remove();
                }
            }
        }

        function showCodeViewPanel() {
            const vis = document.getElementById('visualization');
            
            if (document.getElementById('codeViewPanel')) {
                document.getElementById('codeViewPanel').remove();
                return;
            }
            
            const codePanel = document.createElement('div');
            codePanel.id = 'codeViewPanel';
            codePanel.className = 'code-view-panel';
            codePanel.innerHTML = `
                <div class="code-tabs">
                    <button class="code-tab active" onclick="showCodeTab('pytorch')">PyTorch</button>
                    <button class="code-tab" onclick="showCodeTab('tensorflow')">TensorFlow</button>
                    <button class="code-tab" onclick="showCodeTab('pseudo')">Pseudo-code</button>
                </div>
                
                <div id="codeContent"></div>
            `;
            
            vis.parentNode.insertBefore(codePanel, vis);
            showCodeTab('pytorch');
        }

        function showCodeTab(framework) {
            document.querySelectorAll('.code-tab').forEach(tab => {
                tab.classList.remove('active');
            });
            event.target.classList.add('active');
            
            const codes = {
                pytorch: `<span style="color: #66d9ef;">import</span> torch
<span style="color: #66d9ef;">import</span> torch.nn <span style="color: #66d9ef;">as</span> nn

<span style="color: #66d9ef;">class</span> <span style="color: #a6e22e;">MultiHeadAttention</span>(nn.Module):
    <span style="color: #66d9ef;">def</span> <span style="color: #a6e22e;">__init__</span>(<span style="color: #fd971f;">self</span>, d_model, num_heads):
        <span style="color: #fd971f;">super</span>().__init__()
        <span style="color: #fd971f;">self</span>.d_model = d_model
        <span style="color: #fd971f;">self</span>.num_heads = num_heads
        <span style="color: #fd971f;">self</span>.d_k = d_model // num_heads
        
        <span style="color: #75715e;"># Linear layers for Q, K, V projections</span>
        <span style="color: #fd971f;">self</span>.W_q = nn.Linear(d_model, d_model)
        <span style="color: #fd971f;">self</span>.W_k = nn.Linear(d_model, d_model)
        <span style="color: #fd971f;">self</span>.W_v = nn.Linear(d_model, d_model)
        <span style="color: #fd971f;">self</span>.W_o = nn.Linear(d_model, d_model)
    
    <span style="color: #66d9ef;">def</span> <span style="color: #a6e22e;">forward</span>(<span style="color: #fd971f;">self</span>, x, mask=<span style="color: #66d9ef;">None</span>):
        batch_size = x.size(<span style="color: #ae81ff;">0</span>)
        
        <span style="color: #75715e;"># Linear projections</span>
        Q = <span style="color: #fd971f;">self</span>.W_q(x).view(batch_size, -<span style="color: #ae81ff;">1</span>, <span style="color: #fd971f;">self</span>.num_heads, <span style="color: #fd971f;">self</span>.d_k).transpose(<span style="color: #ae81ff;">1</span>, <span style="color: #ae81ff;">2</span>)
        K = <span style="color: #fd971f;">self</span>.W_k(x).view(batch_size, -<span style="color: #ae81ff;">1</span>, <span style="color: #fd971f;">self</span>.num_heads, <span style="color: #fd971f;">self</span>.d_k).transpose(<span style="color: #ae81ff;">1</span>, <span style="color: #ae81ff;">2</span>)
        V = <span style="color: #fd971f;">self</span>.W_v(x).view(batch_size, -<span style="color: #ae81ff;">1</span>, <span style="color: #fd971f;">self</span>.num_heads, <span style="color: #fd971f;">self</span>.d_k).transpose(<span style="color: #ae81ff;">1</span>, <span style="color: #ae81ff;">2</span>)
        
        <span style="color: #75715e;"># Scaled dot-product attention</span>
        scores = torch.matmul(Q, K.transpose(-<span style="color: #ae81ff;">2</span>, -<span style="color: #ae81ff;">1</span>)) / torch.sqrt(torch.tensor(<span style="color: #fd971f;">self</span>.d_k))
        
        <span style="color: #66d9ef;">if</span> mask <span style="color: #66d9ef;">is</span> <span style="color: #66d9ef;">not</span> <span style="color: #66d9ef;">None</span>:
            scores = scores.masked_fill(mask == <span style="color: #ae81ff;">0</span>, -<span style="color: #ae81ff;">1e9</span>)
        
        attention = torch.softmax(scores, dim=-<span style="color: #ae81ff;">1</span>)
        output = torch.matmul(attention, V)
        
        <span style="color: #75715e;"># Concatenate heads and project</span>
        output = output.transpose(<span style="color: #ae81ff;">1</span>, <span style="color: #ae81ff;">2</span>).contiguous().view(batch_size, -<span style="color: #ae81ff;">1</span>, <span style="color: #fd971f;">self</span>.d_model)
        <span style="color: #66d9ef;">return</span> <span style="color: #fd971f;">self</span>.W_o(output)`,
                
                tensorflow: `<span style="color: #66d9ef;">import</span> tensorflow <span style="color: #66d9ef;">as</span> tf

<span style="color: #66d9ef;">class</span> <span style="color: #a6e22e;">MultiHeadAttention</span>(tf.keras.layers.Layer):
    <span style="color: #66d9ef;">def</span> <span style="color: #a6e22e;">__init__</span>(<span style="color: #fd971f;">self</span>, d_model, num_heads):
        <span style="color: #fd971f;">super</span>().__init__()
        <span style="color: #fd971f;">self</span>.num_heads = num_heads
        <span style="color: #fd971f;">self</span>.d_model = d_model
        <span style="color: #fd971f;">self</span>.d_k = d_model // num_heads
        
        <span style="color: #fd971f;">self</span>.W_q = tf.keras.layers.Dense(d_model)
        <span style="color: #fd971f;">self</span>.W_k = tf.keras.layers.Dense(d_model)
        <span style="color: #fd971f;">self</span>.W_v = tf.keras.layers.Dense(d_model)
        <span style="color: #fd971f;">self</span>.W_o = tf.keras.layers.Dense(d_model)
    
    <span style="color: #66d9ef;">def</span> <span style="color: #a6e22e;">call</span>(<span style="color: #fd971f;">self</span>, x, mask=<span style="color: #66d9ef;">None</span>):
        batch_size = tf.shape(x)[<span style="color: #ae81ff;">0</span>]
        
        Q = <span style="color: #fd971f;">self</span>.split_heads(<span style="color: #fd971f;">self</span>.W_q(x), batch_size)
        K = <span style="color: #fd971f;">self</span>.split_heads(<span style="color: #fd971f;">self</span>.W_k(x), batch_size)
        V = <span style="color: #fd971f;">self</span>.split_heads(<span style="color: #fd971f;">self</span>.W_v(x), batch_size)
        
        scores = tf.matmul(Q, K, transpose_b=<span style="color: #66d9ef;">True</span>) / tf.math.sqrt(tf.cast(<span style="color: #fd971f;">self</span>.d_k, tf.float32))
        
        <span style="color: #66d9ef;">if</span> mask <span style="color: #66d9ef;">is</span> <span style="color: #66d9ef;">not</span> <span style="color: #66d9ef;">None</span>:
            scores += (mask * -<span style="color: #ae81ff;">1e9</span>)
        
        attention = tf.nn.softmax(scores, axis=-<span style="color: #ae81ff;">1</span>)
        output = tf.matmul(attention, V)
        
        output = <span style="color: #fd971f;">self</span>.combine_heads(output, batch_size)
        <span style="color: #66d9ef;">return</span> <span style="color: #fd971f;">self</span>.W_o(output)`,
                
                pseudo: `<span style="color: #75715e;"># Multi-Head Attention Algorithm</span>

<span style="color: #66d9ef;">function</span> <span style="color: #a6e22e;">MultiHeadAttention</span>(X, num_heads):
    <span style="color: #75715e;"># Input: X of shape [batch, seq_len, d_model]</span>
    
    <span style="color: #75715e;"># Step 1: Linear projections</span>
    Q = X @ W_Q  <span style="color: #75715e;"># Query matrix</span>
    K = X @ W_K  <span style="color: #75715e;"># Key matrix</span>
    V = X @ W_V  <span style="color: #75715e;"># Value matrix</span>
    
    <span style="color: #75715e;"># Step 2: Split into multiple heads</span>
    Q_heads = split(Q, num_heads)  <span style="color: #75715e;"># [batch, heads, seq_len, d_k]</span>
    K_heads = split(K, num_heads)
    V_heads = split(V, num_heads)
    
    <span style="color: #75715e;"># Step 3: Scaled dot-product attention for each head</span>
    <span style="color: #66d9ef;">for</span> each head h:
        scores_h = (Q_heads[h] @ K_heads[h]^T) / sqrt(d_k)
        attention_h = softmax(scores_h)
        output_h = attention_h @ V_heads[h]
    
    <span style="color: #75715e;"># Step 4: Concatenate heads</span>
    output = concatenate(output_1, ..., output_h)
    
    <span style="color: #75715e;"># Step 5: Final linear projection</span>
    result = output @ W_O
    
    <span style="color: #66d9ef;">return</span> result`
            };
            
            document.getElementById('codeContent').innerHTML = `<pre style="margin: 0; overflow-x: auto;">${codes[framework]}</pre>`;
        }

        // Debug Mode
        let debugModeEnabled = false;

        function toggleDebugMode() {
            debugModeEnabled = !debugModeEnabled;
            
            if (debugModeEnabled) {
                showDebugPanel();
            } else {
                const debugPanel = document.getElementById('debugPanel');
                if (debugPanel) {
                    debugPanel.remove();
                }
            }
        }

        function showDebugPanel() {
            const vis = document.getElementById('visualization');
            
            if (document.getElementById('debugPanel')) {
                document.getElementById('debugPanel').remove();
                return;
            }
            
            const debugPanel = document.createElement('div');
            debugPanel.id = 'debugPanel';
            debugPanel.className = 'debug-panel';
            debugPanel.innerHTML = `
                <h3 style="color: #4ec9b0; margin-bottom: 20px;">üêõ Debug Mode - Tensor Inspector</h3>
                
                <div style="margin-bottom: 20px;">
                    <button class="btn btn-primary" onclick="stepThroughTransformer()">
                        ‚ñ∂Ô∏è Step Through
                    </button>
                    <button class="btn btn-secondary" onclick="resetDebug()">
                        üîÑ Reset
                    </button>
                </div>
                
                <div id="debugSteps"></div>
            `;
            
            vis.parentNode.insertBefore(debugPanel, vis);
        }

        let debugStep = 0;

        async function stepThroughTransformer() {
            const steps = [
                {
                    name: "Input Tokens",
                    tensor: [[1, 2, 3, 4]],
                    shape: "[1, 4]",
                    description: "Token IDs della sequenza input"
                },
                {
                    name: "Token Embeddings",
                    tensor: [[0.1, -0.3, 0.5], [0.2, 0.4, -0.1], [0.3, -0.2, 0.6], [0.4, 0.1, -0.3]],
                    shape: "[4, 3]",
                    description: "Vettori densi dopo embedding lookup"
                },
                {
                    name: "After Positional Encoding",
                    tensor: [[0.15, -0.25, 0.55], [0.25, 0.45, -0.05], [0.35, -0.15, 0.65], [0.45, 0.15, -0.25]],
                    shape: "[4, 3]",
                    description: "Embeddings + positional encoding"
                },
                {
                    name: "Query Matrix (Q)",
                    tensor: [[0.2, -0.1, 0.3], [0.1, 0.2, -0.2], [0.3, 0.1, 0.4], [0.2, 0.3, -0.1]],
                    shape: "[4, 3]",
                    description: "Matrice Query per self-attention"
                },
                {
                    name: "Attention Scores",
                    tensor: [[0.45, 0.25, 0.15, 0.15], [0.20, 0.50, 0.20, 0.10], [0.15, 0.25, 0.40, 0.20], [0.10, 0.20, 0.30, 0.40]],
                    shape: "[4, 4]",
                    description: "Attention weights dopo softmax"
                },
                {
                    name: "After Self-Attention",
                    tensor: [[0.22, -0.05, 0.35], [0.18, 0.25, -0.08], [0.28, 0.12, 0.42], [0.25, 0.22, -0.05]],
                    shape: "[4, 3]",
                    description: "Output dopo multi-head attention"
                },
                {
                    name: "After Feed-Forward",
                    tensor: [[0.35, -0.15, 0.48], [0.28, 0.38, -0.12], [0.42, 0.22, 0.58], [0.38, 0.32, -0.08]],
                    shape: "[4, 3]",
                    description: "Output dopo feed-forward network"
                },
                {
                    name: "Final Output",
                    tensor: [[0.38, -0.12, 0.52], [0.32, 0.42, -0.10], [0.45, 0.25, 0.62], [0.42, 0.35, -0.05]],
                    shape: "[4, 3]",
                    description: "Output finale dopo layer normalization"
                }
            ];
            
            if (debugStep >= steps.length) {
                debugStep = 0;
                document.getElementById('debugSteps').innerHTML = '';
            }
            
            const step = steps[debugStep];
            const stepsDiv = document.getElementById('debugSteps');
            
            const stepDiv = document.createElement('div');
            stepDiv.className = 'tensor-view';
            stepDiv.style.animation = 'tokenFloat 0.5s ease-in-out';
            stepDiv.innerHTML = `
                <div class="tensor-label">Step ${debugStep + 1}: ${step.name}</div>
                <div style="color: #ce9178; margin: 5px 0;">Shape: ${step.shape}</div>
                <div style="color: #9cdcfe; margin: 10px 0;">${step.description}</div>
                <div class="tensor-values">
                    ${step.tensor.flat().map(val => `
                        <div class="tensor-value">${val.toFixed(2)}</div>
                    `).join('')}
                </div>
            `;
            
            stepsDiv.appendChild(stepDiv);
            debugStep++;
            
            stepDiv.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
        }

        function resetDebug() {
            debugStep = 0;
            document.getElementById('debugSteps').innerHTML = '';
        }

        function sleep(ms) {
            return new Promise(resolve => setTimeout(resolve, ms));
        }

        // Show overview on load
        showOverview();
    </script>
</body>
</html>
